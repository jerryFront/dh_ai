{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "---\n",
      "conv1.weight torch.Size([6, 1, 5, 5])\n",
      "conv1.bias torch.Size([6])\n",
      "conv2.weight torch.Size([16, 6, 5, 5])\n",
      "conv2.bias torch.Size([16])\n",
      "fc1.weight torch.Size([120, 256])\n",
      "fc1.bias torch.Size([120])\n",
      "fc2.weight torch.Size([84, 120])\n",
      "fc2.bias torch.Size([84])\n",
      "fc3.weight torch.Size([10, 84])\n",
      "fc3.bias torch.Size([10])\n",
      "total parameters: 44426\n",
      "input x to model: torch.Size([64, 1, 28, 28])\n",
      "conv1 after size: torch.Size([64, 6, 24, 24])\n",
      "pool1 after size: torch.Size([64, 6, 12, 12])\n",
      "conv2 after size: torch.Size([64, 16, 8, 8])\n",
      "pool2 after size: torch.Size([64, 16, 4, 4])\n",
      "fc1 after size: torch.Size([64, 120])\n",
      "fc2 after size: torch.Size([64, 84])\n",
      "fc3 after size: torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input channel, 6 output channels, 5x5 kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # 6 input channels, 6 output channels, 5x5 kernel\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # 6 input channels, 16 output channels, 5x5 kernel\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # 16 input channels, 16 output channels, 5x5 kernel\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        x = self.conv1(x)  # input shape: (batch_size, 1, 28, 28)\n",
    "        x = torch.relu(x)  # output shape: (batch_size, 6, 24, 24)\n",
    "        x = self.pool1(x)  # output shape: (batch_size, 6, 12, 12)\n",
    "\n",
    "        x = self.conv2(x)  # input shape: (batch_size, 6, 12, 12)\n",
    "        x = torch.relu(x)  # output shape: (batch_size, 16, 8, 8)\n",
    "        x = self.pool2(x)  # output shape: (batch_size, 16, 4, 4)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)  # output shape: (batch_size, 16*4*4)\n",
    "        x = self.fc1(x)  # output shape: (batch_size, 120)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)  # output shape: (batch_size, 84)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)  # output shape: (batch_size, 10)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(x)\n",
    "            prob = torch.softmax(output, dim=1)\n",
    "            prob_list = prob.squeeze().tolist()\n",
    "            prob, index = torch.max(prob, 1)\n",
    "        return index, prob, prob_list\n",
    "\n",
    "\n",
    "def print_params(model):\n",
    "    count = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        count += param.numel()\n",
    "        print(name, param.shape)\n",
    "    print('total parameters:', count)\n",
    "\n",
    "\n",
    "def print_forward(model, x):\n",
    "    print('input x to model:', x.shape)\n",
    "    for name, layer in model.named_children():\n",
    "        if name == 'fc1':\n",
    "            x = x.view(x.shape[0], -1)\n",
    "        x = layer(x)\n",
    "        print(f'{name} after size: {x.shape}')\n",
    "\n",
    "\n",
    "test_model = LeNet()\n",
    "print(test_model)\n",
    "print('---')\n",
    "print_params(test_model)\n",
    "\n",
    "x = torch.randn(64, 1, 28, 28)\n",
    "print_forward(test_model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 设置全局随机种子\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(seed + worker_id)\n",
    "    random.seed(seed + worker_id)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(12),  # 随机旋转 12 度\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 加载训练数据集\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "# 定义训练数据加载器\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset, batch_size=64, shuffle=True, worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "model = LeNet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/938], Loss: 0.6048\n",
      "Epoch [1/100], Step [200/938], Loss: 0.4350\n",
      "Epoch [1/100], Step [300/938], Loss: 0.2492\n",
      "Epoch [1/100], Step [400/938], Loss: 0.2758\n",
      "Epoch [1/100], Step [500/938], Loss: 0.1600\n",
      "Epoch [1/100], Step [600/938], Loss: 0.1157\n",
      "Epoch [1/100], Step [700/938], Loss: 0.2832\n",
      "Epoch [1/100], Step [800/938], Loss: 0.0978\n",
      "Epoch [1/100], Step [900/938], Loss: 0.1411\n",
      "Epoch [2/100], Step [100/938], Loss: 0.0956\n",
      "Epoch [2/100], Step [200/938], Loss: 0.0547\n",
      "Epoch [2/100], Step [300/938], Loss: 0.0584\n",
      "Epoch [2/100], Step [400/938], Loss: 0.1105\n",
      "Epoch [2/100], Step [500/938], Loss: 0.0100\n",
      "Epoch [2/100], Step [600/938], Loss: 0.0322\n",
      "Epoch [2/100], Step [700/938], Loss: 0.0795\n",
      "Epoch [2/100], Step [800/938], Loss: 0.0537\n",
      "Epoch [2/100], Step [900/938], Loss: 0.0379\n",
      "Epoch [3/100], Step [100/938], Loss: 0.1153\n",
      "Epoch [3/100], Step [200/938], Loss: 0.0156\n",
      "Epoch [3/100], Step [300/938], Loss: 0.1798\n",
      "Epoch [3/100], Step [400/938], Loss: 0.0555\n",
      "Epoch [3/100], Step [500/938], Loss: 0.1087\n",
      "Epoch [3/100], Step [600/938], Loss: 0.0044\n",
      "Epoch [3/100], Step [700/938], Loss: 0.0479\n",
      "Epoch [3/100], Step [800/938], Loss: 0.2854\n",
      "Epoch [3/100], Step [900/938], Loss: 0.0636\n",
      "Epoch [4/100], Step [100/938], Loss: 0.0436\n",
      "Epoch [4/100], Step [200/938], Loss: 0.0461\n",
      "Epoch [4/100], Step [300/938], Loss: 0.0976\n",
      "Epoch [4/100], Step [400/938], Loss: 0.0316\n",
      "Epoch [4/100], Step [500/938], Loss: 0.0696\n",
      "Epoch [4/100], Step [600/938], Loss: 0.0792\n",
      "Epoch [4/100], Step [700/938], Loss: 0.0084\n",
      "Epoch [4/100], Step [800/938], Loss: 0.0243\n",
      "Epoch [4/100], Step [900/938], Loss: 0.0978\n",
      "Epoch [5/100], Step [100/938], Loss: 0.0382\n",
      "Epoch [5/100], Step [200/938], Loss: 0.2321\n",
      "Epoch [5/100], Step [300/938], Loss: 0.0281\n",
      "Epoch [5/100], Step [400/938], Loss: 0.0698\n",
      "Epoch [5/100], Step [500/938], Loss: 0.0363\n",
      "Epoch [5/100], Step [600/938], Loss: 0.1197\n",
      "Epoch [5/100], Step [700/938], Loss: 0.1125\n",
      "Epoch [5/100], Step [800/938], Loss: 0.1292\n",
      "Epoch [5/100], Step [900/938], Loss: 0.0056\n",
      "Epoch [6/100], Step [100/938], Loss: 0.0088\n",
      "Epoch [6/100], Step [200/938], Loss: 0.0297\n",
      "Epoch [6/100], Step [300/938], Loss: 0.0685\n",
      "Epoch [6/100], Step [400/938], Loss: 0.0251\n",
      "Epoch [6/100], Step [500/938], Loss: 0.0100\n",
      "Epoch [6/100], Step [600/938], Loss: 0.0363\n",
      "Epoch [6/100], Step [700/938], Loss: 0.0625\n",
      "Epoch [6/100], Step [800/938], Loss: 0.0302\n",
      "Epoch [6/100], Step [900/938], Loss: 0.0182\n",
      "Epoch [7/100], Step [100/938], Loss: 0.0189\n",
      "Epoch [7/100], Step [200/938], Loss: 0.0184\n",
      "Epoch [7/100], Step [300/938], Loss: 0.0440\n",
      "Epoch [7/100], Step [400/938], Loss: 0.0949\n",
      "Epoch [7/100], Step [500/938], Loss: 0.0622\n",
      "Epoch [7/100], Step [600/938], Loss: 0.0206\n",
      "Epoch [7/100], Step [700/938], Loss: 0.0026\n",
      "Epoch [7/100], Step [800/938], Loss: 0.0380\n",
      "Epoch [7/100], Step [900/938], Loss: 0.1267\n",
      "Epoch [8/100], Step [100/938], Loss: 0.0298\n",
      "Epoch [8/100], Step [200/938], Loss: 0.0248\n",
      "Epoch [8/100], Step [300/938], Loss: 0.0093\n",
      "Epoch [8/100], Step [400/938], Loss: 0.0563\n",
      "Epoch [8/100], Step [500/938], Loss: 0.0098\n",
      "Epoch [8/100], Step [600/938], Loss: 0.0097\n",
      "Epoch [8/100], Step [700/938], Loss: 0.0150\n",
      "Epoch [8/100], Step [800/938], Loss: 0.0538\n",
      "Epoch [8/100], Step [900/938], Loss: 0.0124\n",
      "Epoch [9/100], Step [100/938], Loss: 0.0020\n",
      "Epoch [9/100], Step [200/938], Loss: 0.0071\n",
      "Epoch [9/100], Step [300/938], Loss: 0.0099\n",
      "Epoch [9/100], Step [400/938], Loss: 0.0604\n",
      "Epoch [9/100], Step [500/938], Loss: 0.0006\n",
      "Epoch [9/100], Step [600/938], Loss: 0.0027\n",
      "Epoch [9/100], Step [700/938], Loss: 0.0095\n",
      "Epoch [9/100], Step [800/938], Loss: 0.0608\n",
      "Epoch [9/100], Step [900/938], Loss: 0.0515\n",
      "Epoch [10/100], Step [100/938], Loss: 0.0081\n",
      "Epoch [10/100], Step [200/938], Loss: 0.0551\n",
      "Epoch [10/100], Step [300/938], Loss: 0.0506\n",
      "Epoch [10/100], Step [400/938], Loss: 0.0183\n",
      "Epoch [10/100], Step [500/938], Loss: 0.2351\n",
      "Epoch [10/100], Step [600/938], Loss: 0.1268\n",
      "Epoch [10/100], Step [700/938], Loss: 0.0216\n",
      "Epoch [10/100], Step [800/938], Loss: 0.0025\n",
      "Epoch [10/100], Step [900/938], Loss: 0.0515\n",
      "Epoch [11/100], Step [100/938], Loss: 0.0279\n",
      "Epoch [11/100], Step [200/938], Loss: 0.1261\n",
      "Epoch [11/100], Step [300/938], Loss: 0.0136\n",
      "Epoch [11/100], Step [400/938], Loss: 0.0334\n",
      "Epoch [11/100], Step [500/938], Loss: 0.2129\n",
      "Epoch [11/100], Step [600/938], Loss: 0.0675\n",
      "Epoch [11/100], Step [700/938], Loss: 0.0303\n",
      "Epoch [11/100], Step [800/938], Loss: 0.0018\n",
      "Epoch [11/100], Step [900/938], Loss: 0.1202\n",
      "Epoch [12/100], Step [100/938], Loss: 0.0147\n",
      "Epoch [12/100], Step [200/938], Loss: 0.0665\n",
      "Epoch [12/100], Step [300/938], Loss: 0.0034\n",
      "Epoch [12/100], Step [400/938], Loss: 0.0019\n",
      "Epoch [12/100], Step [500/938], Loss: 0.0371\n",
      "Epoch [12/100], Step [600/938], Loss: 0.0024\n",
      "Epoch [12/100], Step [700/938], Loss: 0.0287\n",
      "Epoch [12/100], Step [800/938], Loss: 0.0160\n",
      "Epoch [12/100], Step [900/938], Loss: 0.0182\n",
      "Epoch [13/100], Step [100/938], Loss: 0.1182\n",
      "Epoch [13/100], Step [200/938], Loss: 0.0038\n",
      "Epoch [13/100], Step [300/938], Loss: 0.1747\n",
      "Epoch [13/100], Step [400/938], Loss: 0.1455\n",
      "Epoch [13/100], Step [500/938], Loss: 0.0316\n",
      "Epoch [13/100], Step [600/938], Loss: 0.0732\n",
      "Epoch [13/100], Step [700/938], Loss: 0.0464\n",
      "Epoch [13/100], Step [800/938], Loss: 0.0022\n",
      "Epoch [13/100], Step [900/938], Loss: 0.0004\n",
      "Epoch [14/100], Step [100/938], Loss: 0.0506\n",
      "Epoch [14/100], Step [200/938], Loss: 0.0043\n",
      "Epoch [14/100], Step [300/938], Loss: 0.0766\n",
      "Epoch [14/100], Step [400/938], Loss: 0.1622\n",
      "Epoch [14/100], Step [500/938], Loss: 0.0040\n",
      "Epoch [14/100], Step [600/938], Loss: 0.0206\n",
      "Epoch [14/100], Step [700/938], Loss: 0.0655\n",
      "Epoch [14/100], Step [800/938], Loss: 0.0059\n",
      "Epoch [14/100], Step [900/938], Loss: 0.0126\n",
      "Epoch [15/100], Step [100/938], Loss: 0.0332\n",
      "Epoch [15/100], Step [200/938], Loss: 0.0095\n",
      "Epoch [15/100], Step [300/938], Loss: 0.0042\n",
      "Epoch [15/100], Step [400/938], Loss: 0.0012\n",
      "Epoch [15/100], Step [500/938], Loss: 0.0013\n",
      "Epoch [15/100], Step [600/938], Loss: 0.0309\n",
      "Epoch [15/100], Step [700/938], Loss: 0.0190\n",
      "Epoch [15/100], Step [800/938], Loss: 0.0825\n",
      "Epoch [15/100], Step [900/938], Loss: 0.0031\n",
      "Epoch [16/100], Step [100/938], Loss: 0.0047\n",
      "Epoch [16/100], Step [200/938], Loss: 0.0171\n",
      "Epoch [16/100], Step [300/938], Loss: 0.0024\n",
      "Epoch [16/100], Step [400/938], Loss: 0.0839\n",
      "Epoch [16/100], Step [500/938], Loss: 0.0109\n",
      "Epoch [16/100], Step [600/938], Loss: 0.0017\n",
      "Epoch [16/100], Step [700/938], Loss: 0.0008\n",
      "Epoch [16/100], Step [800/938], Loss: 0.0263\n",
      "Epoch [16/100], Step [900/938], Loss: 0.0001\n",
      "Epoch [17/100], Step [100/938], Loss: 0.0010\n",
      "Epoch [17/100], Step [200/938], Loss: 0.0042\n",
      "Epoch [17/100], Step [300/938], Loss: 0.0326\n",
      "Epoch [17/100], Step [400/938], Loss: 0.0211\n",
      "Epoch [17/100], Step [500/938], Loss: 0.0105\n",
      "Epoch [17/100], Step [600/938], Loss: 0.0158\n",
      "Epoch [17/100], Step [700/938], Loss: 0.0005\n",
      "Epoch [17/100], Step [800/938], Loss: 0.0004\n",
      "Epoch [17/100], Step [900/938], Loss: 0.0138\n",
      "Epoch [18/100], Step [100/938], Loss: 0.0022\n",
      "Epoch [18/100], Step [200/938], Loss: 0.0519\n",
      "Epoch [18/100], Step [300/938], Loss: 0.0004\n",
      "Epoch [18/100], Step [400/938], Loss: 0.0215\n",
      "Epoch [18/100], Step [500/938], Loss: 0.0694\n",
      "Epoch [18/100], Step [600/938], Loss: 0.0043\n",
      "Epoch [18/100], Step [700/938], Loss: 0.0022\n",
      "Epoch [18/100], Step [800/938], Loss: 0.0042\n",
      "Epoch [18/100], Step [900/938], Loss: 0.0093\n",
      "Epoch [19/100], Step [100/938], Loss: 0.0065\n",
      "Epoch [19/100], Step [200/938], Loss: 0.0060\n",
      "Epoch [19/100], Step [300/938], Loss: 0.0487\n",
      "Epoch [19/100], Step [400/938], Loss: 0.0001\n",
      "Epoch [19/100], Step [500/938], Loss: 0.0051\n",
      "Epoch [19/100], Step [600/938], Loss: 0.0097\n",
      "Epoch [19/100], Step [700/938], Loss: 0.0007\n",
      "Epoch [19/100], Step [800/938], Loss: 0.0163\n",
      "Epoch [19/100], Step [900/938], Loss: 0.0126\n",
      "Epoch [20/100], Step [100/938], Loss: 0.0237\n",
      "Epoch [20/100], Step [200/938], Loss: 0.0100\n",
      "Epoch [20/100], Step [300/938], Loss: 0.0045\n",
      "Epoch [20/100], Step [400/938], Loss: 0.0014\n",
      "Epoch [20/100], Step [500/938], Loss: 0.0352\n",
      "Epoch [20/100], Step [600/938], Loss: 0.0369\n",
      "Epoch [20/100], Step [700/938], Loss: 0.0088\n",
      "Epoch [20/100], Step [800/938], Loss: 0.0111\n",
      "Epoch [20/100], Step [900/938], Loss: 0.0051\n",
      "Epoch [21/100], Step [100/938], Loss: 0.0128\n",
      "Epoch [21/100], Step [200/938], Loss: 0.0092\n",
      "Epoch [21/100], Step [300/938], Loss: 0.0015\n",
      "Epoch [21/100], Step [400/938], Loss: 0.0017\n",
      "Epoch [21/100], Step [500/938], Loss: 0.0007\n",
      "Epoch [21/100], Step [600/938], Loss: 0.0031\n",
      "Epoch [21/100], Step [700/938], Loss: 0.0018\n",
      "Epoch [21/100], Step [800/938], Loss: 0.0169\n",
      "Epoch [21/100], Step [900/938], Loss: 0.0005\n",
      "Epoch [22/100], Step [100/938], Loss: 0.0889\n",
      "Epoch [22/100], Step [200/938], Loss: 0.0018\n",
      "Epoch [22/100], Step [300/938], Loss: 0.0010\n",
      "Epoch [22/100], Step [400/938], Loss: 0.0002\n",
      "Epoch [22/100], Step [500/938], Loss: 0.0891\n",
      "Epoch [22/100], Step [600/938], Loss: 0.0039\n",
      "Epoch [22/100], Step [700/938], Loss: 0.1023\n",
      "Epoch [22/100], Step [800/938], Loss: 0.0022\n",
      "Epoch [22/100], Step [900/938], Loss: 0.0221\n",
      "Epoch [23/100], Step [100/938], Loss: 0.0007\n",
      "Epoch [23/100], Step [200/938], Loss: 0.0059\n",
      "Epoch [23/100], Step [300/938], Loss: 0.1319\n",
      "Epoch [23/100], Step [400/938], Loss: 0.0018\n",
      "Epoch [23/100], Step [500/938], Loss: 0.0006\n",
      "Epoch [23/100], Step [600/938], Loss: 0.0264\n",
      "Epoch [23/100], Step [700/938], Loss: 0.0297\n",
      "Epoch [23/100], Step [800/938], Loss: 0.0244\n",
      "Epoch [23/100], Step [900/938], Loss: 0.0014\n",
      "Epoch [24/100], Step [100/938], Loss: 0.0097\n",
      "Epoch [24/100], Step [200/938], Loss: 0.0257\n",
      "Epoch [24/100], Step [300/938], Loss: 0.0304\n",
      "Epoch [24/100], Step [400/938], Loss: 0.0002\n",
      "Epoch [24/100], Step [500/938], Loss: 0.0095\n",
      "Epoch [24/100], Step [600/938], Loss: 0.0924\n",
      "Epoch [24/100], Step [700/938], Loss: 0.0904\n",
      "Epoch [24/100], Step [800/938], Loss: 0.0022\n",
      "Epoch [24/100], Step [900/938], Loss: 0.0008\n",
      "Epoch [25/100], Step [100/938], Loss: 0.0022\n",
      "Epoch [25/100], Step [200/938], Loss: 0.0419\n",
      "Epoch [25/100], Step [300/938], Loss: 0.0509\n",
      "Epoch [25/100], Step [400/938], Loss: 0.0217\n",
      "Epoch [25/100], Step [500/938], Loss: 0.0074\n",
      "Epoch [25/100], Step [600/938], Loss: 0.0302\n",
      "Epoch [25/100], Step [700/938], Loss: 0.0002\n",
      "Epoch [25/100], Step [800/938], Loss: 0.0326\n",
      "Epoch [25/100], Step [900/938], Loss: 0.0132\n",
      "Epoch [26/100], Step [100/938], Loss: 0.0008\n",
      "Epoch [26/100], Step [200/938], Loss: 0.0044\n",
      "Epoch [26/100], Step [300/938], Loss: 0.0004\n",
      "Epoch [26/100], Step [400/938], Loss: 0.0004\n",
      "Epoch [26/100], Step [500/938], Loss: 0.0004\n",
      "Epoch [26/100], Step [600/938], Loss: 0.0063\n",
      "Epoch [26/100], Step [700/938], Loss: 0.0007\n",
      "Epoch [26/100], Step [800/938], Loss: 0.0474\n",
      "Epoch [26/100], Step [900/938], Loss: 0.0047\n",
      "Epoch [27/100], Step [100/938], Loss: 0.0055\n",
      "Epoch [27/100], Step [200/938], Loss: 0.0014\n",
      "Epoch [27/100], Step [300/938], Loss: 0.0006\n",
      "Epoch [27/100], Step [400/938], Loss: 0.0047\n",
      "Epoch [27/100], Step [500/938], Loss: 0.0048\n",
      "Epoch [27/100], Step [600/938], Loss: 0.0019\n",
      "Epoch [27/100], Step [700/938], Loss: 0.0001\n",
      "Epoch [27/100], Step [800/938], Loss: 0.0056\n",
      "Epoch [27/100], Step [900/938], Loss: 0.0004\n",
      "Epoch [28/100], Step [100/938], Loss: 0.0328\n",
      "Epoch [28/100], Step [200/938], Loss: 0.0558\n",
      "Epoch [28/100], Step [300/938], Loss: 0.0010\n",
      "Epoch [28/100], Step [400/938], Loss: 0.0116\n",
      "Epoch [28/100], Step [500/938], Loss: 0.0011\n",
      "Epoch [28/100], Step [600/938], Loss: 0.0094\n",
      "Epoch [28/100], Step [700/938], Loss: 0.0004\n",
      "Epoch [28/100], Step [800/938], Loss: 0.0045\n",
      "Epoch [28/100], Step [900/938], Loss: 0.0177\n",
      "Epoch [29/100], Step [100/938], Loss: 0.0002\n",
      "Epoch [29/100], Step [200/938], Loss: 0.0020\n",
      "Epoch [29/100], Step [300/938], Loss: 0.0003\n",
      "Epoch [29/100], Step [400/938], Loss: 0.0088\n",
      "Epoch [29/100], Step [500/938], Loss: 0.0055\n",
      "Epoch [29/100], Step [600/938], Loss: 0.0076\n",
      "Epoch [29/100], Step [700/938], Loss: 0.0857\n",
      "Epoch [29/100], Step [800/938], Loss: 0.0001\n",
      "Epoch [29/100], Step [900/938], Loss: 0.0107\n",
      "Epoch [30/100], Step [100/938], Loss: 0.0091\n",
      "Epoch [30/100], Step [200/938], Loss: 0.0143\n",
      "Epoch [30/100], Step [300/938], Loss: 0.0011\n",
      "Epoch [30/100], Step [400/938], Loss: 0.0003\n",
      "Epoch [30/100], Step [500/938], Loss: 0.0110\n",
      "Epoch [30/100], Step [600/938], Loss: 0.0002\n",
      "Epoch [30/100], Step [700/938], Loss: 0.0577\n",
      "Epoch [30/100], Step [800/938], Loss: 0.0675\n",
      "Epoch [30/100], Step [900/938], Loss: 0.0561\n",
      "Epoch [31/100], Step [100/938], Loss: 0.0001\n",
      "Epoch [31/100], Step [200/938], Loss: 0.0024\n",
      "Epoch [31/100], Step [300/938], Loss: 0.0219\n",
      "Epoch [31/100], Step [400/938], Loss: 0.0116\n",
      "Epoch [31/100], Step [500/938], Loss: 0.0033\n",
      "Epoch [31/100], Step [600/938], Loss: 0.0016\n",
      "Epoch [31/100], Step [700/938], Loss: 0.0046\n",
      "Epoch [31/100], Step [800/938], Loss: 0.0002\n",
      "Epoch [31/100], Step [900/938], Loss: 0.0028\n",
      "Epoch [32/100], Step [100/938], Loss: 0.0105\n",
      "Epoch [32/100], Step [200/938], Loss: 0.0063\n",
      "Epoch [32/100], Step [300/938], Loss: 0.0066\n",
      "Epoch [32/100], Step [400/938], Loss: 0.0002\n",
      "Epoch [32/100], Step [500/938], Loss: 0.0147\n",
      "Epoch [32/100], Step [600/938], Loss: 0.0002\n",
      "Epoch [32/100], Step [700/938], Loss: 0.0013\n",
      "Epoch [32/100], Step [800/938], Loss: 0.0687\n",
      "Epoch [32/100], Step [900/938], Loss: 0.0506\n",
      "Epoch [33/100], Step [100/938], Loss: 0.0021\n",
      "Epoch [33/100], Step [200/938], Loss: 0.0016\n",
      "Epoch [33/100], Step [300/938], Loss: 0.0001\n",
      "Epoch [33/100], Step [400/938], Loss: 0.0028\n",
      "Epoch [33/100], Step [500/938], Loss: 0.0027\n",
      "Epoch [33/100], Step [600/938], Loss: 0.0203\n",
      "Epoch [33/100], Step [700/938], Loss: 0.0042\n",
      "Epoch [33/100], Step [800/938], Loss: 0.0007\n",
      "Epoch [33/100], Step [900/938], Loss: 0.0025\n",
      "Epoch [34/100], Step [100/938], Loss: 0.0007\n",
      "Epoch [34/100], Step [200/938], Loss: 0.0005\n",
      "Epoch [34/100], Step [300/938], Loss: 0.0221\n",
      "Epoch [34/100], Step [400/938], Loss: 0.0368\n",
      "Epoch [34/100], Step [500/938], Loss: 0.0006\n",
      "Epoch [34/100], Step [600/938], Loss: 0.0039\n",
      "Epoch [34/100], Step [700/938], Loss: 0.0012\n",
      "Epoch [34/100], Step [800/938], Loss: 0.0009\n",
      "Epoch [34/100], Step [900/938], Loss: 0.0604\n",
      "Epoch [35/100], Step [100/938], Loss: 0.0021\n",
      "Epoch [35/100], Step [200/938], Loss: 0.0030\n",
      "Epoch [35/100], Step [300/938], Loss: 0.0322\n",
      "Epoch [35/100], Step [400/938], Loss: 0.0017\n",
      "Epoch [35/100], Step [500/938], Loss: 0.0001\n",
      "Epoch [35/100], Step [600/938], Loss: 0.0003\n",
      "Epoch [35/100], Step [700/938], Loss: 0.0056\n",
      "Epoch [35/100], Step [800/938], Loss: 0.0001\n",
      "Epoch [35/100], Step [900/938], Loss: 0.0065\n",
      "Epoch [36/100], Step [100/938], Loss: 0.0117\n",
      "Epoch [36/100], Step [200/938], Loss: 0.0025\n",
      "Epoch [36/100], Step [300/938], Loss: 0.0072\n",
      "Epoch [36/100], Step [400/938], Loss: 0.0004\n",
      "Epoch [36/100], Step [500/938], Loss: 0.0192\n",
      "Epoch [36/100], Step [600/938], Loss: 0.0038\n",
      "Epoch [36/100], Step [700/938], Loss: 0.0034\n",
      "Epoch [36/100], Step [800/938], Loss: 0.0356\n",
      "Epoch [36/100], Step [900/938], Loss: 0.0028\n",
      "Epoch [37/100], Step [100/938], Loss: 0.1149\n",
      "Epoch [37/100], Step [200/938], Loss: 0.0011\n",
      "Epoch [37/100], Step [300/938], Loss: 0.0064\n",
      "Epoch [37/100], Step [400/938], Loss: 0.0006\n",
      "Epoch [37/100], Step [500/938], Loss: 0.0149\n",
      "Epoch [37/100], Step [600/938], Loss: 0.0011\n",
      "Epoch [37/100], Step [700/938], Loss: 0.0032\n",
      "Epoch [37/100], Step [800/938], Loss: 0.0068\n",
      "Epoch [37/100], Step [900/938], Loss: 0.0010\n",
      "Epoch [38/100], Step [100/938], Loss: 0.0004\n",
      "Epoch [38/100], Step [200/938], Loss: 0.0193\n",
      "Epoch [38/100], Step [300/938], Loss: 0.0176\n",
      "Epoch [38/100], Step [400/938], Loss: 0.0017\n",
      "Epoch [38/100], Step [500/938], Loss: 0.0071\n",
      "Epoch [38/100], Step [600/938], Loss: 0.1119\n",
      "Epoch [38/100], Step [700/938], Loss: 0.0002\n",
      "Epoch [38/100], Step [800/938], Loss: 0.0321\n",
      "Epoch [38/100], Step [900/938], Loss: 0.0027\n",
      "Epoch [39/100], Step [100/938], Loss: 0.0028\n",
      "Epoch [39/100], Step [200/938], Loss: 0.0018\n",
      "Epoch [39/100], Step [300/938], Loss: 0.0001\n",
      "Epoch [39/100], Step [400/938], Loss: 0.0754\n",
      "Epoch [39/100], Step [500/938], Loss: 0.0006\n",
      "Epoch [39/100], Step [600/938], Loss: 0.0001\n",
      "Epoch [39/100], Step [700/938], Loss: 0.0071\n",
      "Epoch [39/100], Step [800/938], Loss: 0.0002\n",
      "Epoch [39/100], Step [900/938], Loss: 0.0003\n",
      "Epoch [40/100], Step [100/938], Loss: 0.0399\n",
      "Epoch [40/100], Step [200/938], Loss: 0.0251\n",
      "Epoch [40/100], Step [300/938], Loss: 0.0016\n",
      "Epoch [40/100], Step [400/938], Loss: 0.0027\n",
      "Epoch [40/100], Step [500/938], Loss: 0.0039\n",
      "Epoch [40/100], Step [600/938], Loss: 0.0003\n",
      "Epoch [40/100], Step [700/938], Loss: 0.0171\n",
      "Epoch [40/100], Step [800/938], Loss: 0.0022\n",
      "Epoch [40/100], Step [900/938], Loss: 0.0044\n",
      "Epoch [41/100], Step [100/938], Loss: 0.0000\n",
      "Epoch [41/100], Step [200/938], Loss: 0.0112\n",
      "Epoch [41/100], Step [300/938], Loss: 0.0092\n",
      "Epoch [41/100], Step [400/938], Loss: 0.0012\n",
      "Epoch [41/100], Step [500/938], Loss: 0.0001\n",
      "Epoch [41/100], Step [600/938], Loss: 0.0035\n",
      "Epoch [41/100], Step [700/938], Loss: 0.1991\n",
      "Epoch [41/100], Step [800/938], Loss: 0.0223\n",
      "Epoch [41/100], Step [900/938], Loss: 0.0961\n",
      "Epoch [42/100], Step [100/938], Loss: 0.0128\n",
      "Epoch [42/100], Step [200/938], Loss: 0.0008\n",
      "Epoch [42/100], Step [300/938], Loss: 0.0025\n",
      "Epoch [42/100], Step [400/938], Loss: 0.0002\n",
      "Epoch [42/100], Step [500/938], Loss: 0.0010\n",
      "Epoch [42/100], Step [600/938], Loss: 0.0447\n",
      "Epoch [42/100], Step [700/938], Loss: 0.0183\n",
      "Epoch [42/100], Step [800/938], Loss: 0.0012\n",
      "Epoch [42/100], Step [900/938], Loss: 0.0058\n",
      "Epoch [43/100], Step [100/938], Loss: 0.0001\n",
      "Epoch [43/100], Step [200/938], Loss: 0.0004\n",
      "Epoch [43/100], Step [300/938], Loss: 0.0024\n",
      "Epoch [43/100], Step [400/938], Loss: 0.0009\n",
      "Epoch [43/100], Step [500/938], Loss: 0.0000\n",
      "Epoch [43/100], Step [600/938], Loss: 0.0012\n",
      "Epoch [43/100], Step [700/938], Loss: 0.0018\n",
      "Epoch [43/100], Step [800/938], Loss: 0.0292\n",
      "Epoch [43/100], Step [900/938], Loss: 0.0108\n",
      "Epoch [44/100], Step [100/938], Loss: 0.0004\n",
      "Epoch [44/100], Step [200/938], Loss: 0.0102\n",
      "Epoch [44/100], Step [300/938], Loss: 0.0001\n",
      "Epoch [44/100], Step [400/938], Loss: 0.0127\n",
      "Epoch [44/100], Step [500/938], Loss: 0.0003\n",
      "Epoch [44/100], Step [600/938], Loss: 0.0519\n",
      "Epoch [44/100], Step [700/938], Loss: 0.0712\n",
      "Epoch [44/100], Step [800/938], Loss: 0.0219\n",
      "Epoch [44/100], Step [900/938], Loss: 0.0185\n",
      "Epoch [45/100], Step [100/938], Loss: 0.0083\n",
      "Epoch [45/100], Step [200/938], Loss: 0.0053\n",
      "Epoch [45/100], Step [300/938], Loss: 0.0014\n",
      "Epoch [45/100], Step [400/938], Loss: 0.0035\n",
      "Epoch [45/100], Step [500/938], Loss: 0.1487\n",
      "Epoch [45/100], Step [600/938], Loss: 0.0013\n",
      "Epoch [45/100], Step [700/938], Loss: 0.0043\n",
      "Epoch [45/100], Step [800/938], Loss: 0.0017\n",
      "Epoch [45/100], Step [900/938], Loss: 0.0014\n",
      "Epoch [46/100], Step [100/938], Loss: 0.0013\n",
      "Epoch [46/100], Step [200/938], Loss: 0.0022\n",
      "Epoch [46/100], Step [300/938], Loss: 0.0036\n",
      "Epoch [46/100], Step [400/938], Loss: 0.0068\n",
      "Epoch [46/100], Step [500/938], Loss: 0.0006\n",
      "Epoch [46/100], Step [600/938], Loss: 0.0004\n",
      "Epoch [46/100], Step [700/938], Loss: 0.0009\n",
      "Epoch [46/100], Step [800/938], Loss: 0.0330\n",
      "Epoch [46/100], Step [900/938], Loss: 0.0932\n",
      "Epoch [47/100], Step [100/938], Loss: 0.0000\n",
      "Epoch [47/100], Step [200/938], Loss: 0.0012\n",
      "Epoch [47/100], Step [300/938], Loss: 0.0014\n",
      "Epoch [47/100], Step [400/938], Loss: 0.0111\n",
      "Epoch [47/100], Step [500/938], Loss: 0.0004\n",
      "Epoch [47/100], Step [600/938], Loss: 0.0870\n",
      "Epoch [47/100], Step [700/938], Loss: 0.0267\n",
      "Epoch [47/100], Step [800/938], Loss: 0.0001\n",
      "Epoch [47/100], Step [900/938], Loss: 0.0287\n",
      "Epoch [48/100], Step [100/938], Loss: 0.0186\n",
      "Epoch [48/100], Step [200/938], Loss: 0.0008\n",
      "Epoch [48/100], Step [300/938], Loss: 0.0030\n",
      "Epoch [48/100], Step [400/938], Loss: 0.0002\n",
      "Epoch [48/100], Step [500/938], Loss: 0.0003\n",
      "Epoch [48/100], Step [600/938], Loss: 0.0118\n",
      "Epoch [48/100], Step [700/938], Loss: 0.0003\n",
      "Epoch [48/100], Step [800/938], Loss: 0.0002\n",
      "Epoch [48/100], Step [900/938], Loss: 0.0086\n",
      "Epoch [49/100], Step [100/938], Loss: 0.0005\n",
      "Epoch [49/100], Step [200/938], Loss: 0.0046\n",
      "Epoch [49/100], Step [300/938], Loss: 0.0086\n",
      "Epoch [49/100], Step [400/938], Loss: 0.0038\n",
      "Epoch [49/100], Step [500/938], Loss: 0.1085\n",
      "Epoch [49/100], Step [600/938], Loss: 0.0002\n",
      "Epoch [49/100], Step [700/938], Loss: 0.0007\n",
      "Epoch [49/100], Step [800/938], Loss: 0.0000\n",
      "Epoch [49/100], Step [900/938], Loss: 0.0004\n",
      "Epoch [50/100], Step [100/938], Loss: 0.0010\n",
      "Epoch [50/100], Step [200/938], Loss: 0.0259\n",
      "Epoch [50/100], Step [300/938], Loss: 0.0004\n",
      "Epoch [50/100], Step [400/938], Loss: 0.0003\n",
      "Epoch [50/100], Step [500/938], Loss: 0.0004\n",
      "Epoch [50/100], Step [600/938], Loss: 0.0001\n",
      "Epoch [50/100], Step [700/938], Loss: 0.0005\n",
      "Epoch [50/100], Step [800/938], Loss: 0.0002\n",
      "Epoch [50/100], Step [900/938], Loss: 0.0257\n",
      "Epoch [51/100], Step [100/938], Loss: 0.0610\n",
      "Epoch [51/100], Step [200/938], Loss: 0.0017\n",
      "Epoch [51/100], Step [300/938], Loss: 0.0204\n",
      "Epoch [51/100], Step [400/938], Loss: 0.0130\n",
      "Epoch [51/100], Step [500/938], Loss: 0.0000\n",
      "Epoch [51/100], Step [600/938], Loss: 0.0174\n",
      "Epoch [51/100], Step [700/938], Loss: 0.0837\n",
      "Epoch [51/100], Step [800/938], Loss: 0.0024\n",
      "Epoch [51/100], Step [900/938], Loss: 0.0387\n",
      "Epoch [52/100], Step [100/938], Loss: 0.0045\n",
      "Epoch [52/100], Step [200/938], Loss: 0.0085\n",
      "Epoch [52/100], Step [300/938], Loss: 0.0021\n",
      "Epoch [52/100], Step [400/938], Loss: 0.0390\n",
      "Epoch [52/100], Step [500/938], Loss: 0.0514\n",
      "Epoch [52/100], Step [600/938], Loss: 0.0001\n",
      "Epoch [52/100], Step [700/938], Loss: 0.0000\n",
      "Epoch [52/100], Step [800/938], Loss: 0.0002\n",
      "Epoch [52/100], Step [900/938], Loss: 0.0174\n",
      "Epoch [53/100], Step [100/938], Loss: 0.0003\n",
      "Epoch [53/100], Step [200/938], Loss: 0.0302\n",
      "Epoch [53/100], Step [300/938], Loss: 0.0001\n",
      "Epoch [53/100], Step [400/938], Loss: 0.0001\n",
      "Epoch [53/100], Step [500/938], Loss: 0.0022\n",
      "Epoch [53/100], Step [600/938], Loss: 0.0017\n",
      "Epoch [53/100], Step [700/938], Loss: 0.0000\n",
      "Epoch [53/100], Step [800/938], Loss: 0.0006\n",
      "Epoch [53/100], Step [900/938], Loss: 0.0741\n",
      "Epoch [54/100], Step [100/938], Loss: 0.0001\n",
      "Epoch [54/100], Step [200/938], Loss: 0.0582\n",
      "Epoch [54/100], Step [300/938], Loss: 0.0022\n",
      "Epoch [54/100], Step [400/938], Loss: 0.0020\n",
      "Epoch [54/100], Step [500/938], Loss: 0.0002\n",
      "Epoch [54/100], Step [600/938], Loss: 0.0001\n",
      "Epoch [54/100], Step [700/938], Loss: 0.0168\n",
      "Epoch [54/100], Step [800/938], Loss: 0.0155\n",
      "Epoch [54/100], Step [900/938], Loss: 0.0000\n",
      "Epoch [55/100], Step [100/938], Loss: 0.0002\n",
      "Epoch [55/100], Step [200/938], Loss: 0.0158\n",
      "Epoch [55/100], Step [300/938], Loss: 0.0005\n",
      "Epoch [55/100], Step [400/938], Loss: 0.0004\n",
      "Epoch [55/100], Step [500/938], Loss: 0.0114\n",
      "Epoch [55/100], Step [600/938], Loss: 0.0013\n",
      "Epoch [55/100], Step [700/938], Loss: 0.0000\n",
      "Epoch [55/100], Step [800/938], Loss: 0.0045\n",
      "Epoch [55/100], Step [900/938], Loss: 0.2088\n",
      "Epoch [56/100], Step [100/938], Loss: 0.0149\n",
      "Epoch [56/100], Step [200/938], Loss: 0.0000\n",
      "Epoch [56/100], Step [300/938], Loss: 0.0000\n",
      "Epoch [56/100], Step [400/938], Loss: 0.0003\n",
      "Epoch [56/100], Step [500/938], Loss: 0.0025\n",
      "Epoch [56/100], Step [600/938], Loss: 0.0015\n",
      "Epoch [56/100], Step [700/938], Loss: 0.0010\n",
      "Epoch [56/100], Step [800/938], Loss: 0.0001\n",
      "Epoch [56/100], Step [900/938], Loss: 0.0004\n",
      "Epoch [57/100], Step [100/938], Loss: 0.0068\n",
      "Epoch [57/100], Step [200/938], Loss: 0.0002\n",
      "Epoch [57/100], Step [300/938], Loss: 0.0018\n",
      "Epoch [57/100], Step [400/938], Loss: 0.0023\n",
      "Epoch [57/100], Step [500/938], Loss: 0.0175\n",
      "Epoch [57/100], Step [600/938], Loss: 0.0022\n",
      "Epoch [57/100], Step [700/938], Loss: 0.0190\n",
      "Epoch [57/100], Step [800/938], Loss: 0.0002\n",
      "Epoch [57/100], Step [900/938], Loss: 0.0198\n",
      "Epoch [58/100], Step [100/938], Loss: 0.0105\n",
      "Epoch [58/100], Step [200/938], Loss: 0.0020\n",
      "Epoch [58/100], Step [300/938], Loss: 0.0011\n",
      "Epoch [58/100], Step [400/938], Loss: 0.0003\n",
      "Epoch [58/100], Step [500/938], Loss: 0.0033\n",
      "Epoch [58/100], Step [600/938], Loss: 0.0129\n",
      "Epoch [58/100], Step [700/938], Loss: 0.0002\n",
      "Epoch [58/100], Step [800/938], Loss: 0.0000\n",
      "Epoch [58/100], Step [900/938], Loss: 0.0001\n",
      "Epoch [59/100], Step [100/938], Loss: 0.0003\n",
      "Epoch [59/100], Step [200/938], Loss: 0.0000\n",
      "Epoch [59/100], Step [300/938], Loss: 0.0000\n",
      "Epoch [59/100], Step [400/938], Loss: 0.0007\n",
      "Epoch [59/100], Step [500/938], Loss: 0.0001\n",
      "Epoch [59/100], Step [600/938], Loss: 0.0010\n",
      "Epoch [59/100], Step [700/938], Loss: 0.0004\n",
      "Epoch [59/100], Step [800/938], Loss: 0.0000\n",
      "Epoch [59/100], Step [900/938], Loss: 0.0005\n",
      "Epoch [60/100], Step [100/938], Loss: 0.0031\n",
      "Epoch [60/100], Step [200/938], Loss: 0.0000\n",
      "Epoch [60/100], Step [300/938], Loss: 0.0230\n",
      "Epoch [60/100], Step [400/938], Loss: 0.0564\n",
      "Epoch [60/100], Step [500/938], Loss: 0.0134\n",
      "Epoch [60/100], Step [600/938], Loss: 0.0040\n",
      "Epoch [60/100], Step [700/938], Loss: 0.0066\n",
      "Epoch [60/100], Step [800/938], Loss: 0.0009\n",
      "Epoch [60/100], Step [900/938], Loss: 0.0078\n",
      "Epoch [61/100], Step [100/938], Loss: 0.0014\n",
      "Epoch [61/100], Step [200/938], Loss: 0.0006\n",
      "Epoch [61/100], Step [300/938], Loss: 0.0154\n",
      "Epoch [61/100], Step [400/938], Loss: 0.0026\n",
      "Epoch [61/100], Step [500/938], Loss: 0.0298\n",
      "Epoch [61/100], Step [600/938], Loss: 0.0215\n",
      "Epoch [61/100], Step [700/938], Loss: 0.0017\n",
      "Epoch [61/100], Step [800/938], Loss: 0.0050\n",
      "Epoch [61/100], Step [900/938], Loss: 0.0004\n",
      "Epoch [62/100], Step [100/938], Loss: 0.0016\n",
      "Epoch [62/100], Step [200/938], Loss: 0.0010\n",
      "Epoch [62/100], Step [300/938], Loss: 0.0012\n",
      "Epoch [62/100], Step [400/938], Loss: 0.0119\n",
      "Epoch [62/100], Step [500/938], Loss: 0.0078\n",
      "Epoch [62/100], Step [600/938], Loss: 0.0001\n",
      "Epoch [62/100], Step [700/938], Loss: 0.0021\n",
      "Epoch [62/100], Step [800/938], Loss: 0.0001\n",
      "Epoch [62/100], Step [900/938], Loss: 0.0038\n",
      "Epoch [63/100], Step [100/938], Loss: 0.0023\n",
      "Epoch [63/100], Step [200/938], Loss: 0.0002\n",
      "Epoch [63/100], Step [300/938], Loss: 0.0002\n",
      "Epoch [63/100], Step [400/938], Loss: 0.0000\n",
      "Epoch [63/100], Step [500/938], Loss: 0.0001\n",
      "Epoch [63/100], Step [600/938], Loss: 0.0000\n",
      "Epoch [63/100], Step [700/938], Loss: 0.0005\n",
      "Epoch [63/100], Step [800/938], Loss: 0.0052\n",
      "Epoch [63/100], Step [900/938], Loss: 0.0158\n",
      "Epoch [64/100], Step [100/938], Loss: 0.0000\n",
      "Epoch [64/100], Step [200/938], Loss: 0.0014\n",
      "Epoch [64/100], Step [300/938], Loss: 0.0040\n",
      "Epoch [64/100], Step [400/938], Loss: 0.0112\n",
      "Epoch [64/100], Step [500/938], Loss: 0.0118\n",
      "Epoch [64/100], Step [600/938], Loss: 0.0000\n",
      "Epoch [64/100], Step [700/938], Loss: 0.0013\n",
      "Epoch [64/100], Step [800/938], Loss: 0.0003\n",
      "Epoch [64/100], Step [900/938], Loss: 0.0116\n",
      "Epoch [65/100], Step [100/938], Loss: 0.0009\n",
      "Epoch [65/100], Step [200/938], Loss: 0.0032\n",
      "Epoch [65/100], Step [300/938], Loss: 0.0004\n",
      "Epoch [65/100], Step [400/938], Loss: 0.0001\n",
      "Epoch [65/100], Step [500/938], Loss: 0.0044\n",
      "Epoch [65/100], Step [600/938], Loss: 0.0420\n",
      "Epoch [65/100], Step [700/938], Loss: 0.0520\n",
      "Epoch [65/100], Step [800/938], Loss: 0.0006\n",
      "Epoch [65/100], Step [900/938], Loss: 0.0050\n",
      "Epoch [66/100], Step [100/938], Loss: 0.0025\n",
      "Epoch [66/100], Step [200/938], Loss: 0.0003\n",
      "Epoch [66/100], Step [300/938], Loss: 0.0000\n",
      "Epoch [66/100], Step [400/938], Loss: 0.0009\n",
      "Epoch [66/100], Step [500/938], Loss: 0.0001\n",
      "Epoch [66/100], Step [600/938], Loss: 0.0011\n",
      "Epoch [66/100], Step [700/938], Loss: 0.0003\n",
      "Epoch [66/100], Step [800/938], Loss: 0.0003\n",
      "Epoch [66/100], Step [900/938], Loss: 0.0002\n",
      "Epoch [67/100], Step [100/938], Loss: 0.0001\n",
      "Epoch [67/100], Step [200/938], Loss: 0.0136\n",
      "Epoch [67/100], Step [300/938], Loss: 0.0020\n",
      "Epoch [67/100], Step [400/938], Loss: 0.0000\n",
      "Epoch [67/100], Step [500/938], Loss: 0.0072\n",
      "Epoch [67/100], Step [600/938], Loss: 0.0000\n",
      "Epoch [67/100], Step [700/938], Loss: 0.0000\n",
      "Epoch [67/100], Step [800/938], Loss: 0.0002\n",
      "Epoch [67/100], Step [900/938], Loss: 0.0646\n",
      "Epoch [68/100], Step [100/938], Loss: 0.0004\n",
      "Epoch [68/100], Step [200/938], Loss: 0.0001\n",
      "Epoch [68/100], Step [300/938], Loss: 0.0011\n",
      "Epoch [68/100], Step [400/938], Loss: 0.0001\n",
      "Epoch [68/100], Step [500/938], Loss: 0.0002\n",
      "Epoch [68/100], Step [600/938], Loss: 0.0001\n",
      "Epoch [68/100], Step [700/938], Loss: 0.0005\n",
      "Epoch [68/100], Step [800/938], Loss: 0.0085\n",
      "Epoch [68/100], Step [900/938], Loss: 0.0009\n",
      "Epoch [69/100], Step [100/938], Loss: 0.0001\n",
      "Epoch [69/100], Step [200/938], Loss: 0.0135\n",
      "Epoch [69/100], Step [300/938], Loss: 0.0267\n",
      "Epoch [69/100], Step [400/938], Loss: 0.0125\n",
      "Epoch [69/100], Step [500/938], Loss: 0.0002\n",
      "Epoch [69/100], Step [600/938], Loss: 0.0009\n",
      "Epoch [69/100], Step [700/938], Loss: 0.1378\n",
      "Epoch [69/100], Step [800/938], Loss: 0.0133\n",
      "Epoch [69/100], Step [900/938], Loss: 0.0001\n",
      "Epoch [70/100], Step [100/938], Loss: 0.0000\n",
      "Epoch [70/100], Step [200/938], Loss: 0.0126\n",
      "Epoch [70/100], Step [300/938], Loss: 0.0007\n",
      "Epoch [70/100], Step [400/938], Loss: 0.0017\n",
      "Epoch [70/100], Step [500/938], Loss: 0.0000\n",
      "Epoch [70/100], Step [600/938], Loss: 0.0000\n",
      "Epoch [70/100], Step [700/938], Loss: 0.0005\n",
      "Epoch [70/100], Step [800/938], Loss: 0.0000\n",
      "Epoch [70/100], Step [900/938], Loss: 0.0002\n",
      "Epoch [71/100], Step [100/938], Loss: 0.0009\n",
      "Epoch [71/100], Step [200/938], Loss: 0.0001\n",
      "Epoch [71/100], Step [300/938], Loss: 0.0002\n",
      "Epoch [71/100], Step [400/938], Loss: 0.0001\n",
      "Epoch [71/100], Step [500/938], Loss: 0.0000\n",
      "Epoch [71/100], Step [600/938], Loss: 0.0001\n",
      "Epoch [71/100], Step [700/938], Loss: 0.0093\n",
      "Epoch [71/100], Step [800/938], Loss: 0.0001\n",
      "Epoch [71/100], Step [900/938], Loss: 0.0001\n",
      "Epoch [72/100], Step [100/938], Loss: 0.0001\n",
      "Epoch [72/100], Step [200/938], Loss: 0.0012\n",
      "Epoch [72/100], Step [300/938], Loss: 0.0014\n",
      "Epoch [72/100], Step [400/938], Loss: 0.0017\n",
      "Epoch [72/100], Step [500/938], Loss: 0.0000\n",
      "Epoch [72/100], Step [600/938], Loss: 0.0030\n",
      "Epoch [72/100], Step [700/938], Loss: 0.0033\n",
      "Epoch [72/100], Step [800/938], Loss: 0.0000\n",
      "Epoch [72/100], Step [900/938], Loss: 0.0203\n",
      "Epoch [73/100], Step [100/938], Loss: 0.0164\n",
      "Epoch [73/100], Step [200/938], Loss: 0.0001\n",
      "Epoch [73/100], Step [300/938], Loss: 0.0003\n",
      "Epoch [73/100], Step [400/938], Loss: 0.0002\n",
      "Epoch [73/100], Step [500/938], Loss: 0.0003\n",
      "Epoch [73/100], Step [600/938], Loss: 0.0012\n",
      "Epoch [73/100], Step [700/938], Loss: 0.0014\n",
      "Epoch [73/100], Step [800/938], Loss: 0.0031\n",
      "Epoch [73/100], Step [900/938], Loss: 0.0000\n",
      "Epoch [74/100], Step [100/938], Loss: 0.0000\n",
      "Epoch [74/100], Step [200/938], Loss: 0.0042\n",
      "Epoch [74/100], Step [300/938], Loss: 0.0000\n",
      "Epoch [74/100], Step [400/938], Loss: 0.0001\n",
      "Epoch [74/100], Step [500/938], Loss: 0.0002\n",
      "Epoch [74/100], Step [600/938], Loss: 0.0019\n",
      "Epoch [74/100], Step [700/938], Loss: 0.0006\n",
      "Epoch [74/100], Step [800/938], Loss: 0.0002\n",
      "Epoch [74/100], Step [900/938], Loss: 0.0417\n",
      "Epoch [75/100], Step [100/938], Loss: 0.1145\n",
      "Epoch [75/100], Step [200/938], Loss: 0.0000\n",
      "Epoch [75/100], Step [300/938], Loss: 0.0000\n",
      "Epoch [75/100], Step [400/938], Loss: 0.0016\n",
      "Epoch [75/100], Step [500/938], Loss: 0.0009\n",
      "Epoch [75/100], Step [600/938], Loss: 0.0008\n",
      "Epoch [75/100], Step [700/938], Loss: 0.0033\n",
      "Epoch [75/100], Step [800/938], Loss: 0.0250\n",
      "Epoch [75/100], Step [900/938], Loss: 0.0022\n",
      "Epoch [76/100], Step [100/938], Loss: 0.0099\n",
      "Epoch [76/100], Step [200/938], Loss: 0.0008\n",
      "Epoch [76/100], Step [300/938], Loss: 0.0002\n",
      "Epoch [76/100], Step [400/938], Loss: 0.0848\n",
      "Epoch [76/100], Step [500/938], Loss: 0.0011\n",
      "Epoch [76/100], Step [600/938], Loss: 0.0000\n",
      "Epoch [76/100], Step [700/938], Loss: 0.0092\n",
      "Epoch [76/100], Step [800/938], Loss: 0.0001\n",
      "Epoch [76/100], Step [900/938], Loss: 0.0001\n",
      "Epoch [77/100], Step [100/938], Loss: 0.0001\n",
      "Epoch [77/100], Step [200/938], Loss: 0.0001\n",
      "Epoch [77/100], Step [300/938], Loss: 0.0002\n",
      "Epoch [77/100], Step [400/938], Loss: 0.0012\n",
      "Epoch [77/100], Step [500/938], Loss: 0.0000\n",
      "Epoch [77/100], Step [600/938], Loss: 0.0001\n",
      "Epoch [77/100], Step [700/938], Loss: 0.1292\n",
      "Epoch [77/100], Step [800/938], Loss: 0.0004\n",
      "Epoch [77/100], Step [900/938], Loss: 0.0006\n",
      "Epoch [78/100], Step [100/938], Loss: 0.0022\n",
      "Epoch [78/100], Step [200/938], Loss: 0.0001\n",
      "Epoch [78/100], Step [300/938], Loss: 0.0000\n",
      "Epoch [78/100], Step [400/938], Loss: 0.0003\n",
      "Epoch [78/100], Step [500/938], Loss: 0.0019\n",
      "Epoch [78/100], Step [600/938], Loss: 0.0000\n",
      "Epoch [78/100], Step [700/938], Loss: 0.0004\n",
      "Epoch [78/100], Step [800/938], Loss: 0.0081\n",
      "Epoch [78/100], Step [900/938], Loss: 0.0066\n",
      "Epoch [79/100], Step [100/938], Loss: 0.0154\n",
      "Epoch [79/100], Step [200/938], Loss: 0.0001\n",
      "Epoch [79/100], Step [300/938], Loss: 0.0026\n",
      "Epoch [79/100], Step [400/938], Loss: 0.0001\n",
      "Epoch [79/100], Step [500/938], Loss: 0.0020\n",
      "Epoch [79/100], Step [600/938], Loss: 0.0001\n",
      "Epoch [79/100], Step [700/938], Loss: 0.0035\n",
      "Epoch [79/100], Step [800/938], Loss: 0.0001\n",
      "Epoch [79/100], Step [900/938], Loss: 0.0006\n",
      "Epoch [80/100], Step [100/938], Loss: 0.0000\n",
      "Epoch [80/100], Step [200/938], Loss: 0.0745\n",
      "Epoch [80/100], Step [300/938], Loss: 0.0305\n",
      "Epoch [80/100], Step [400/938], Loss: 0.0016\n",
      "Epoch [80/100], Step [500/938], Loss: 0.0004\n",
      "Epoch [80/100], Step [600/938], Loss: 0.0024\n",
      "Epoch [80/100], Step [700/938], Loss: 0.0000\n",
      "Epoch [80/100], Step [800/938], Loss: 0.0002\n",
      "Epoch [80/100], Step [900/938], Loss: 0.0000\n",
      "Epoch [81/100], Step [100/938], Loss: 0.0001\n",
      "Epoch [81/100], Step [200/938], Loss: 0.0006\n",
      "Epoch [81/100], Step [300/938], Loss: 0.0001\n",
      "Epoch [81/100], Step [400/938], Loss: 0.0008\n",
      "Epoch [81/100], Step [500/938], Loss: 0.0000\n",
      "Epoch [81/100], Step [600/938], Loss: 0.0000\n",
      "Epoch [81/100], Step [700/938], Loss: 0.0008\n",
      "Epoch [81/100], Step [800/938], Loss: 0.0067\n",
      "Epoch [81/100], Step [900/938], Loss: 0.0002\n",
      "Epoch [82/100], Step [100/938], Loss: 0.0002\n",
      "Epoch [82/100], Step [200/938], Loss: 0.0000\n",
      "Epoch [82/100], Step [300/938], Loss: 0.0138\n",
      "Epoch [82/100], Step [400/938], Loss: 0.0017\n",
      "Epoch [82/100], Step [500/938], Loss: 0.1568\n",
      "Epoch [82/100], Step [600/938], Loss: 0.0001\n",
      "Epoch [82/100], Step [700/938], Loss: 0.0000\n",
      "Epoch [82/100], Step [800/938], Loss: 0.0000\n",
      "Epoch [82/100], Step [900/938], Loss: 0.0004\n",
      "Epoch [83/100], Step [100/938], Loss: 0.0030\n",
      "Epoch [83/100], Step [200/938], Loss: 0.0008\n",
      "Epoch [83/100], Step [300/938], Loss: 0.0011\n",
      "Epoch [83/100], Step [400/938], Loss: 0.0002\n",
      "Epoch [83/100], Step [500/938], Loss: 0.0103\n",
      "Epoch [83/100], Step [600/938], Loss: 0.0019\n",
      "Epoch [83/100], Step [700/938], Loss: 0.0001\n",
      "Epoch [83/100], Step [800/938], Loss: 0.0589\n",
      "Epoch [83/100], Step [900/938], Loss: 0.0024\n",
      "Epoch [84/100], Step [100/938], Loss: 0.0633\n",
      "Epoch [84/100], Step [200/938], Loss: 0.0008\n",
      "Epoch [84/100], Step [300/938], Loss: 0.0000\n",
      "Epoch [84/100], Step [400/938], Loss: 0.0145\n",
      "Epoch [84/100], Step [500/938], Loss: 0.0011\n",
      "Epoch [84/100], Step [600/938], Loss: 0.0000\n",
      "Epoch [84/100], Step [700/938], Loss: 0.0008\n",
      "Epoch [84/100], Step [800/938], Loss: 0.0002\n",
      "Epoch [84/100], Step [900/938], Loss: 0.0093\n",
      "Epoch [85/100], Step [100/938], Loss: 0.0001\n",
      "Epoch [85/100], Step [200/938], Loss: 0.0010\n",
      "Epoch [85/100], Step [300/938], Loss: 0.0005\n",
      "Epoch [85/100], Step [400/938], Loss: 0.0273\n",
      "Epoch [85/100], Step [500/938], Loss: 0.0007\n",
      "Epoch [85/100], Step [600/938], Loss: 0.0114\n",
      "Epoch [85/100], Step [700/938], Loss: 0.0017\n",
      "Epoch [85/100], Step [800/938], Loss: 0.0002\n",
      "Epoch [85/100], Step [900/938], Loss: 0.0002\n",
      "Epoch [86/100], Step [100/938], Loss: 0.0000\n",
      "Epoch [86/100], Step [200/938], Loss: 0.0185\n",
      "Epoch [86/100], Step [300/938], Loss: 0.0010\n",
      "Epoch [86/100], Step [400/938], Loss: 0.0001\n",
      "Epoch [86/100], Step [500/938], Loss: 0.0155\n",
      "Epoch [86/100], Step [600/938], Loss: 0.0001\n",
      "Epoch [86/100], Step [700/938], Loss: 0.0058\n",
      "Epoch [86/100], Step [800/938], Loss: 0.0012\n",
      "Epoch [86/100], Step [900/938], Loss: 0.0000\n",
      "Epoch [87/100], Step [100/938], Loss: 0.0001\n",
      "Epoch [87/100], Step [200/938], Loss: 0.0307\n",
      "Epoch [87/100], Step [300/938], Loss: 0.0000\n",
      "Epoch [87/100], Step [400/938], Loss: 0.0127\n",
      "Epoch [87/100], Step [500/938], Loss: 0.0070\n",
      "Epoch [87/100], Step [600/938], Loss: 0.0010\n",
      "Epoch [87/100], Step [700/938], Loss: 0.0132\n",
      "Epoch [87/100], Step [800/938], Loss: 0.0037\n",
      "Epoch [87/100], Step [900/938], Loss: 0.0004\n",
      "Epoch [88/100], Step [100/938], Loss: 0.0003\n",
      "Epoch [88/100], Step [200/938], Loss: 0.0019\n",
      "Epoch [88/100], Step [300/938], Loss: 0.0006\n",
      "Epoch [88/100], Step [400/938], Loss: 0.0004\n",
      "Epoch [88/100], Step [500/938], Loss: 0.0032\n",
      "Epoch [88/100], Step [600/938], Loss: 0.0000\n",
      "Epoch [88/100], Step [700/938], Loss: 0.0022\n",
      "Epoch [88/100], Step [800/938], Loss: 0.0071\n",
      "Epoch [88/100], Step [900/938], Loss: 0.0015\n",
      "Epoch [89/100], Step [100/938], Loss: 0.0002\n",
      "Epoch [89/100], Step [200/938], Loss: 0.0000\n",
      "Epoch [89/100], Step [300/938], Loss: 0.0000\n",
      "Epoch [89/100], Step [400/938], Loss: 0.0005\n",
      "Epoch [89/100], Step [500/938], Loss: 0.0031\n",
      "Epoch [89/100], Step [600/938], Loss: 0.0000\n",
      "Epoch [89/100], Step [700/938], Loss: 0.0000\n",
      "Epoch [89/100], Step [800/938], Loss: 0.0011\n",
      "Epoch [89/100], Step [900/938], Loss: 0.0008\n",
      "Epoch [90/100], Step [100/938], Loss: 0.0000\n",
      "Epoch [90/100], Step [200/938], Loss: 0.0000\n",
      "Epoch [90/100], Step [300/938], Loss: 0.0003\n",
      "Epoch [90/100], Step [400/938], Loss: 0.0001\n",
      "Epoch [90/100], Step [500/938], Loss: 0.0155\n",
      "Epoch [90/100], Step [600/938], Loss: 0.0095\n",
      "Epoch [90/100], Step [700/938], Loss: 0.0033\n",
      "Epoch [90/100], Step [800/938], Loss: 0.0447\n",
      "Epoch [90/100], Step [900/938], Loss: 0.0000\n",
      "Epoch [91/100], Step [100/938], Loss: 0.0182\n",
      "Epoch [91/100], Step [200/938], Loss: 0.0002\n",
      "Epoch [91/100], Step [300/938], Loss: 0.0023\n",
      "Epoch [91/100], Step [400/938], Loss: 0.0022\n",
      "Epoch [91/100], Step [500/938], Loss: 0.0006\n",
      "Epoch [91/100], Step [600/938], Loss: 0.0000\n",
      "Epoch [91/100], Step [700/938], Loss: 0.0003\n",
      "Epoch [91/100], Step [800/938], Loss: 0.0464\n",
      "Epoch [91/100], Step [900/938], Loss: 0.0004\n",
      "Epoch [92/100], Step [100/938], Loss: 0.0161\n",
      "Epoch [92/100], Step [200/938], Loss: 0.0015\n",
      "Epoch [92/100], Step [300/938], Loss: 0.0004\n",
      "Epoch [92/100], Step [400/938], Loss: 0.0000\n",
      "Epoch [92/100], Step [500/938], Loss: 0.0003\n",
      "Epoch [92/100], Step [600/938], Loss: 0.0001\n",
      "Epoch [92/100], Step [700/938], Loss: 0.0000\n",
      "Epoch [92/100], Step [800/938], Loss: 0.0003\n",
      "Epoch [92/100], Step [900/938], Loss: 0.0000\n",
      "Epoch [93/100], Step [100/938], Loss: 0.0164\n",
      "Epoch [93/100], Step [200/938], Loss: 0.0009\n",
      "Epoch [93/100], Step [300/938], Loss: 0.0000\n",
      "Epoch [93/100], Step [400/938], Loss: 0.0079\n",
      "Epoch [93/100], Step [500/938], Loss: 0.0034\n",
      "Epoch [93/100], Step [600/938], Loss: 0.0000\n",
      "Epoch [93/100], Step [700/938], Loss: 0.0000\n",
      "Epoch [93/100], Step [800/938], Loss: 0.0016\n",
      "Epoch [93/100], Step [900/938], Loss: 0.0011\n",
      "Epoch [94/100], Step [100/938], Loss: 0.0000\n",
      "Epoch [94/100], Step [200/938], Loss: 0.0042\n",
      "Epoch [94/100], Step [300/938], Loss: 0.0007\n",
      "Epoch [94/100], Step [400/938], Loss: 0.1363\n",
      "Epoch [94/100], Step [500/938], Loss: 0.0012\n",
      "Epoch [94/100], Step [600/938], Loss: 0.0002\n",
      "Epoch [94/100], Step [700/938], Loss: 0.0007\n",
      "Epoch [94/100], Step [800/938], Loss: 0.0076\n",
      "Epoch [94/100], Step [900/938], Loss: 0.0000\n",
      "Epoch [95/100], Step [100/938], Loss: 0.0038\n",
      "Epoch [95/100], Step [200/938], Loss: 0.0074\n",
      "Epoch [95/100], Step [300/938], Loss: 0.0008\n",
      "Epoch [95/100], Step [400/938], Loss: 0.0083\n",
      "Epoch [95/100], Step [500/938], Loss: 0.0003\n",
      "Epoch [95/100], Step [600/938], Loss: 0.0286\n",
      "Epoch [95/100], Step [700/938], Loss: 0.0000\n",
      "Epoch [95/100], Step [800/938], Loss: 0.0040\n",
      "Epoch [95/100], Step [900/938], Loss: 0.0018\n",
      "Epoch [96/100], Step [100/938], Loss: 0.0000\n",
      "Epoch [96/100], Step [200/938], Loss: 0.0001\n",
      "Epoch [96/100], Step [300/938], Loss: 0.0000\n",
      "Epoch [96/100], Step [400/938], Loss: 0.0008\n",
      "Epoch [96/100], Step [500/938], Loss: 0.0001\n",
      "Epoch [96/100], Step [600/938], Loss: 0.0008\n",
      "Epoch [96/100], Step [700/938], Loss: 0.0119\n",
      "Epoch [96/100], Step [800/938], Loss: 0.0000\n",
      "Epoch [96/100], Step [900/938], Loss: 0.0000\n",
      "Epoch [97/100], Step [100/938], Loss: 0.0004\n",
      "Epoch [97/100], Step [200/938], Loss: 0.0001\n",
      "Epoch [97/100], Step [300/938], Loss: 0.0031\n",
      "Epoch [97/100], Step [400/938], Loss: 0.0015\n",
      "Epoch [97/100], Step [500/938], Loss: 0.0030\n",
      "Epoch [97/100], Step [600/938], Loss: 0.0115\n",
      "Epoch [97/100], Step [700/938], Loss: 0.0002\n",
      "Epoch [97/100], Step [800/938], Loss: 0.0265\n",
      "Epoch [97/100], Step [900/938], Loss: 0.0000\n",
      "Epoch [98/100], Step [100/938], Loss: 0.0084\n",
      "Epoch [98/100], Step [200/938], Loss: 0.0001\n",
      "Epoch [98/100], Step [300/938], Loss: 0.0058\n",
      "Epoch [98/100], Step [400/938], Loss: 0.0001\n",
      "Epoch [98/100], Step [500/938], Loss: 0.0015\n",
      "Epoch [98/100], Step [600/938], Loss: 0.0824\n",
      "Epoch [98/100], Step [700/938], Loss: 0.0011\n",
      "Epoch [98/100], Step [800/938], Loss: 0.0019\n",
      "Epoch [98/100], Step [900/938], Loss: 0.0001\n",
      "Epoch [99/100], Step [100/938], Loss: 0.0017\n",
      "Epoch [99/100], Step [200/938], Loss: 0.0006\n",
      "Epoch [99/100], Step [300/938], Loss: 0.0000\n",
      "Epoch [99/100], Step [400/938], Loss: 0.0004\n",
      "Epoch [99/100], Step [500/938], Loss: 0.0003\n",
      "Epoch [99/100], Step [600/938], Loss: 0.0001\n",
      "Epoch [99/100], Step [700/938], Loss: 0.0000\n",
      "Epoch [99/100], Step [800/938], Loss: 0.0002\n",
      "Epoch [99/100], Step [900/938], Loss: 0.0064\n",
      "Epoch [100/100], Step [100/938], Loss: 0.0000\n",
      "Epoch [100/100], Step [200/938], Loss: 0.0061\n",
      "Epoch [100/100], Step [300/938], Loss: 0.0831\n",
      "Epoch [100/100], Step [400/938], Loss: 0.0784\n",
      "Epoch [100/100], Step [500/938], Loss: 0.0006\n",
      "Epoch [100/100], Step [600/938], Loss: 0.0012\n",
      "Epoch [100/100], Step [700/938], Loss: 0.0030\n",
      "Epoch [100/100], Step [800/938], Loss: 0.0156\n",
      "Epoch [100/100], Step [900/938], Loss: 0.0060\n"
     ]
    }
   ],
   "source": [
    "# import wandb\n",
    "# wandb.require(\"core\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            # wandb.log({\"loss\": loss.item(), \"acc\": acc.item()})\n",
    "            print(\n",
    "                f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4d/14nkpqsn4zs8c1dmkl0p4mxr0000gp/T/ipykernel_65232/703398484.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LeNet()\n",
    "model.load_state_dict(torch.load(\n",
    "    './model.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5, 1: 5, 2: 5, 3: 11, 4: 7, 5: 14, 6: 11, 7: 8, 8: 15, 9: 20}\n",
      "error-count: 101\n",
      "Accuracy of the network on the 10000 test images: 98.99%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# 加载测试数据集\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 测试模型\n",
    "model.eval()  # 切换到评估模式\n",
    "with torch.no_grad():\n",
    "    \"\"\" 统计每个数字的错误次数 \"\"\"\n",
    "    error_count_dict = {\n",
    "        0: 0,\n",
    "        1: 0,\n",
    "        2: 0,\n",
    "        3: 0,\n",
    "        4: 0,\n",
    "        5: 0,\n",
    "        6: 0,\n",
    "        7: 0,\n",
    "        8: 0,\n",
    "        9: 0\n",
    "    }\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_index, (images, labels) in enumerate(test_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        error_index = np.where((predicted == labels).cpu().numpy() == False)[0]\n",
    "        for index in error_index:\n",
    "            error_count_dict[labels[index].item()] += 1\n",
    "\n",
    "test_loss /= total\n",
    "test_accuracy = correct / total\n",
    "\n",
    "pprint(error_count_dict)\n",
    "print('error-count:', sum(error_count_dict.values()))\n",
    "print('Accuracy of the network on the 10000 test images: {:.2f}%'.format(\n",
    "    100 * test_accuracy))\n",
    "\n",
    "# 将每个数字的错误次数绘制为柱状图\n",
    "plt.bar(error_count_dict.keys(), error_count_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存后加载模型\n",
    "torch.save(model.state_dict(), './model.pth')\n",
    "\n",
    "# model = Net()\n",
    "# model.load_state_dict(torch.load('./server/model.pth'))\n",
    "\n",
    "# # 创建一个示例输入张量（假设输入尺寸是 [1, 784]）\n",
    "# dummy_input = torch.randn(1, 28 * 28)\n",
    "\n",
    "# # 导出模型到 ONNX 格式\n",
    "# torch.onnx.export(\n",
    "#     model,                # 要转换的模型\n",
    "#     dummy_input,          # 示例输入张量\n",
    "#     \"model.onnx\",         # 导出的 ONNX 文件名\n",
    "#     input_names=['input'],  # 输入张量的名称\n",
    "#     output_names=['output'],  # 输出张量的名称\n",
    "#     opset_version=11      # ONNX opset 版本，通常使用最新版本（这里使用 11）\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([6, 1, 5, 5])\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0835, -0.1147,  0.1766, -0.2374, -0.3184],\n",
      "          [ 0.2185, -0.0712,  0.3234, -0.1315, -0.4185],\n",
      "          [ 0.0095,  0.0825,  0.3548,  0.1549, -0.4919],\n",
      "          [-0.1100,  0.1137,  0.3204,  0.2863,  0.1132],\n",
      "          [-0.3496, -0.0276,  0.1008,  0.2028,  0.1251]]],\n",
      "\n",
      "\n",
      "        [[[-0.0621,  0.1024, -0.4355, -0.3109, -0.3119],\n",
      "          [-0.0978, -0.3006, -0.1599, -0.2412, -0.0855],\n",
      "          [ 0.0588, -0.1527,  0.1515,  0.0362,  0.0635],\n",
      "          [ 0.3056,  0.2454,  0.3532,  0.5081,  0.3636],\n",
      "          [-0.0525,  0.1815, -0.0728, -0.1059,  0.0304]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0142,  0.1986,  0.1604,  0.4141,  0.1842],\n",
      "          [ 0.0671,  0.0861,  0.2015,  0.1312,  0.5027],\n",
      "          [-0.2690, -0.1346,  0.1009, -0.1540,  0.0628],\n",
      "          [ 0.0016, -0.0773, -0.3285, -0.2970, -0.3599],\n",
      "          [-0.1304, -0.0544, -0.2001, -0.1619, -0.1192]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1370,  0.3224,  0.1260,  0.0338, -0.1960],\n",
      "          [ 0.3016,  0.1885, -0.0062, -0.3448, -0.3195],\n",
      "          [ 0.2000,  0.3156, -0.0808, -0.4459, -0.3927],\n",
      "          [ 0.1937,  0.0829, -0.2539, -0.3234, -0.3097],\n",
      "          [ 0.1998, -0.0258, -0.0285, -0.2007,  0.0049]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1293,  0.0082, -0.2017, -0.0877,  0.0146],\n",
      "          [-0.1708, -0.1028, -0.0638,  0.1904,  0.1517],\n",
      "          [ 0.0062,  0.2050,  0.3088,  0.2905,  0.1059],\n",
      "          [ 0.0044,  0.4221,  0.1561, -0.2197, -0.2859],\n",
      "          [-0.0078, -0.0156, -0.3512, -0.3174, -0.0434]]],\n",
      "\n",
      "\n",
      "        [[[-0.0473, -0.2301, -0.0649, -0.0621,  0.2681],\n",
      "          [-0.2652, -0.3424, -0.0857,  0.0417,  0.1529],\n",
      "          [-0.4303, -0.1406, -0.2261,  0.3896,  0.3756],\n",
      "          [-0.3348, -0.1118,  0.2039,  0.3735,  0.1749],\n",
      "          [ 0.0477,  0.0948,  0.0268,  0.2343,  0.2681]]]], requires_grad=True)\n",
      "conv1.bias torch.Size([6])\n",
      "Parameter containing:\n",
      "tensor([-0.0685,  0.0153,  0.0203,  0.0580, -0.0192, -0.1412],\n",
      "       requires_grad=True)\n",
      "conv2.weight torch.Size([16, 6, 5, 5])\n",
      "Parameter containing:\n",
      "tensor([[[[-8.9435e-02, -1.9128e-02, -7.2087e-02, -9.3911e-03,  1.9719e-02],\n",
      "          [ 1.3333e-01, -1.1610e-01, -2.9612e-02,  8.7279e-02,  4.3111e-02],\n",
      "          [ 1.9416e-01,  1.4152e-01, -1.8173e-02, -8.3838e-02,  5.8028e-03],\n",
      "          [ 2.4834e-01,  2.7544e-01, -1.1249e-01, -9.4678e-02,  8.5990e-02],\n",
      "          [-1.6954e-01, -3.8346e-02, -5.4788e-03, -1.7399e-02, -1.3478e-01]],\n",
      "\n",
      "         [[ 8.8722e-03,  6.6275e-02, -1.1198e-02,  7.0726e-02,  1.9212e-02],\n",
      "          [ 3.6608e-02,  1.0925e-01,  9.7085e-04, -5.6434e-02,  3.0099e-02],\n",
      "          [-9.2171e-02, -1.2843e-03, -9.8091e-02, -9.2875e-02,  6.6879e-02],\n",
      "          [-3.2779e-02,  1.7255e-01,  2.3392e-01, -5.6378e-02, -1.8962e-02],\n",
      "          [-1.4752e-01,  7.3804e-03, -1.1289e-02, -1.6076e-02, -2.1988e-02]],\n",
      "\n",
      "         [[-5.4964e-03, -3.2849e-02,  9.8963e-02, -6.8725e-02, -4.5245e-02],\n",
      "          [-1.5053e-01, -9.1350e-02, -5.9411e-02, -1.1519e-01, -8.3234e-02],\n",
      "          [-1.7171e-01, -1.6134e-01, -1.3944e-01, -1.3506e-01, -7.8263e-02],\n",
      "          [ 4.9815e-02,  1.9200e-02, -7.4359e-02,  1.9860e-02, -9.5114e-02],\n",
      "          [ 2.9568e-01,  2.2181e-01,  1.6783e-01,  9.6234e-03, -3.4980e-02]],\n",
      "\n",
      "         [[-7.9532e-02,  1.4440e-02,  1.3134e-01,  1.5314e-01,  7.0961e-02],\n",
      "          [-1.8987e-02,  5.7850e-02,  6.1412e-03,  8.0388e-02,  7.5329e-02],\n",
      "          [-1.5532e-01, -1.2438e-01, -2.0917e-01,  4.7874e-02, -5.7518e-02],\n",
      "          [-2.2076e-01, -2.0842e-01, -7.3188e-02, -5.8685e-02,  1.6002e-01],\n",
      "          [-1.0453e-01, -1.3560e-01, -3.2817e-02,  9.4669e-04, -3.9406e-02]],\n",
      "\n",
      "         [[ 6.8841e-02,  3.6492e-02, -1.6834e-02, -3.9106e-02, -1.3207e-01],\n",
      "          [ 1.7031e-03,  2.0293e-02,  2.1116e-02,  1.5307e-02, -3.6611e-02],\n",
      "          [ 8.6769e-02,  1.4292e-01, -7.0923e-03,  1.5437e-01,  2.5253e-02],\n",
      "          [ 1.5759e-02,  1.1194e-01,  1.8736e-01,  7.9332e-02,  8.2918e-02],\n",
      "          [ 3.4274e-02, -1.0234e-02,  1.4069e-01,  4.8623e-02,  3.4304e-02]],\n",
      "\n",
      "         [[-1.7839e-03,  5.9996e-02,  9.6892e-02, -8.7166e-02, -1.3697e-01],\n",
      "          [ 1.3565e-01, -4.7528e-02, -5.4918e-02,  5.5846e-02, -6.7585e-02],\n",
      "          [ 8.9001e-03, -1.3478e-01,  7.2760e-02,  4.3131e-02, -5.9900e-02],\n",
      "          [-6.2208e-02, -8.3432e-02, -1.4144e-01,  7.2006e-02, -1.5503e-01],\n",
      "          [-1.3015e-01, -2.2883e-01, -2.5730e-01, -1.8300e-01, -2.2013e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 7.7175e-02,  1.4646e-01,  7.6474e-03, -1.1711e-01, -7.4342e-02],\n",
      "          [-6.7066e-02,  1.9058e-01,  2.1239e-01, -8.6175e-02,  2.8694e-02],\n",
      "          [-1.5522e-01,  1.4899e-01,  2.0359e-01,  8.6223e-02, -6.5908e-02],\n",
      "          [-2.1165e-01,  1.0011e-01,  8.5449e-02, -8.1217e-02,  1.0960e-02],\n",
      "          [-1.1852e-01, -5.3013e-02, -4.7321e-03, -6.6106e-02, -3.9663e-02]],\n",
      "\n",
      "         [[-1.1765e-01,  3.1923e-02,  3.7676e-02, -4.8729e-02, -7.8680e-02],\n",
      "          [-1.7272e-01, -9.4110e-02, -8.0940e-02, -3.7694e-02, -1.3665e-02],\n",
      "          [-2.5602e-01, -1.2611e-01,  4.4900e-02,  1.9507e-01,  1.1983e-01],\n",
      "          [-1.7377e-01, -2.4306e-01, -9.3680e-02, -3.7142e-02,  2.7672e-03],\n",
      "          [-4.3340e-01, -3.4585e-01, -1.5008e-01, -4.0441e-02, -1.3966e-01]],\n",
      "\n",
      "         [[-3.4267e-01, -8.9899e-02,  5.8326e-02,  1.0266e-01,  3.6523e-02],\n",
      "          [-1.3403e-01, -1.2854e-01, -7.3526e-02, -1.1799e-01,  6.4934e-02],\n",
      "          [-1.0810e-01,  5.1701e-02, -4.9235e-02, -1.2344e-01,  2.3272e-02],\n",
      "          [-9.3004e-02,  1.8705e-01,  6.4625e-02,  5.4094e-02, -1.6917e-02],\n",
      "          [ 3.3341e-02,  2.0014e-02, -5.6766e-02, -6.1040e-02, -2.1926e-02]],\n",
      "\n",
      "         [[-1.4810e-01,  2.8795e-01,  1.6667e-01,  2.8546e-02,  2.7253e-02],\n",
      "          [-1.9524e-01,  1.7789e-01,  2.6302e-01,  8.1743e-02, -8.9551e-02],\n",
      "          [-3.1603e-01,  5.1452e-02,  5.2708e-02, -4.5535e-02, -2.4251e-01],\n",
      "          [-4.6657e-02, -9.4140e-03, -4.5831e-03, -3.1723e-02, -1.2593e-03],\n",
      "          [ 1.0394e-01,  2.8716e-02,  7.5643e-02,  1.0709e-01, -1.8262e-03]],\n",
      "\n",
      "         [[-1.4584e-01,  1.5437e-02,  1.0196e-01,  9.9325e-02,  8.6405e-02],\n",
      "          [-1.9662e-01, -1.0567e-01,  7.3193e-02, -1.0263e-01, -7.3017e-02],\n",
      "          [-2.2588e-02, -1.7544e-02,  1.4800e-01,  5.1756e-02, -6.9420e-02],\n",
      "          [ 3.2544e-02,  2.2310e-02,  3.6879e-02,  4.6362e-02, -9.3635e-02],\n",
      "          [-3.4205e-02, -1.9507e-02, -9.9312e-02, -8.7526e-02, -1.3110e-01]],\n",
      "\n",
      "         [[ 1.6544e-01,  1.4420e-01,  3.2627e-02, -2.7578e-01, -2.6296e-01],\n",
      "          [ 2.6341e-01,  7.1460e-02, -1.7786e-01, -3.0114e-01, -1.4962e-01],\n",
      "          [ 2.1666e-01,  2.1493e-01, -1.9409e-01, -2.2812e-01, -1.4726e-01],\n",
      "          [ 1.1714e-01,  6.1527e-02, -1.0040e-01, -2.6348e-01, -1.4118e-01],\n",
      "          [-8.3457e-02, -1.0421e-01,  3.5930e-03, -1.1003e-02, -6.2841e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.1287e-02,  8.2089e-02,  5.9837e-02, -1.1433e-02,  2.0159e-02],\n",
      "          [ 2.3140e-02,  1.2290e-01,  5.4896e-02,  8.2761e-02,  6.6341e-02],\n",
      "          [ 2.5520e-03, -7.2660e-02, -1.0231e-01,  1.0480e-01, -4.8249e-03],\n",
      "          [ 5.4646e-02,  3.8505e-02,  1.7527e-02,  1.5893e-02, -8.5107e-02],\n",
      "          [ 5.1058e-02,  7.6712e-02, -1.1279e-01, -3.0088e-03,  4.3872e-02]],\n",
      "\n",
      "         [[ 8.3045e-03, -3.5889e-02, -3.8283e-02,  4.4731e-02, -4.9954e-02],\n",
      "          [ 2.3513e-01,  3.5827e-01,  3.0482e-01,  2.8438e-01,  1.0703e-01],\n",
      "          [ 1.2260e-01,  6.9219e-02,  3.6804e-02, -3.2171e-02, -2.2575e-02],\n",
      "          [ 1.5567e-02,  2.8677e-02,  7.2720e-02,  1.4895e-01, -1.7230e-02],\n",
      "          [ 4.5089e-02,  1.4893e-02,  7.2974e-02,  1.8896e-02,  1.6822e-02]],\n",
      "\n",
      "         [[ 6.1974e-02,  3.3564e-02, -1.4958e-01, -2.4659e-01, -9.4587e-02],\n",
      "          [-1.3561e-01, -8.5112e-02, -1.2819e-01, -1.9849e-01, -1.6753e-01],\n",
      "          [ 2.8150e-01,  1.6453e-01,  1.9352e-01,  6.3178e-02, -3.6929e-02],\n",
      "          [-2.0044e-02, -7.8956e-02, -1.6323e-01, -4.4372e-02, -7.7209e-02],\n",
      "          [-4.9615e-02, -4.0515e-02,  8.2819e-02, -7.8303e-02, -4.7256e-02]],\n",
      "\n",
      "         [[-2.4918e-01, -8.2215e-02, -1.3222e-02, -2.9535e-01, -2.2015e-01],\n",
      "          [-4.1226e-01, -4.4910e-01, -3.3597e-01, -3.2233e-01, -2.4477e-01],\n",
      "          [ 2.0378e-02, -1.5851e-01, -1.7450e-01, -1.7986e-01, -1.2883e-01],\n",
      "          [-1.5678e-02, -8.5412e-02, -2.2141e-01, -1.2317e-01, -3.3040e-03],\n",
      "          [-1.1039e-01, -2.6173e-01, -4.9248e-03,  7.0720e-02,  6.8574e-02]],\n",
      "\n",
      "         [[ 2.6995e-02,  8.4959e-03, -1.6677e-01, -2.1515e-01, -1.1712e-01],\n",
      "          [ 1.4861e-01,  2.4650e-01,  1.2135e-01,  1.5774e-01,  3.3144e-02],\n",
      "          [ 5.6572e-02,  1.6476e-01,  1.7744e-01, -1.6365e-02,  1.2847e-02],\n",
      "          [ 2.5202e-02, -6.9489e-02, -6.6395e-02,  5.8679e-02, -4.0021e-03],\n",
      "          [ 5.7712e-02,  3.1875e-02,  4.2512e-02,  8.2064e-02,  4.0655e-02]],\n",
      "\n",
      "         [[-1.1246e-01, -1.4969e-01, -2.9264e-02,  1.1812e-02,  5.0067e-02],\n",
      "          [ 3.0473e-02,  8.9752e-02,  5.7436e-02,  5.2434e-02, -1.6069e-01],\n",
      "          [-5.1634e-02,  1.1231e-03, -2.2257e-02, -2.3719e-01, -2.1931e-01],\n",
      "          [-1.5552e-02, -1.6811e-01, -1.8146e-02,  5.5310e-02, -3.7155e-02],\n",
      "          [-2.4557e-02, -3.6224e-02,  4.0337e-02, -5.6247e-02, -1.3937e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.8247e-02,  2.5995e-02,  7.8819e-02,  9.8917e-02, -5.8210e-02],\n",
      "          [-4.5103e-02, -3.4860e-02, -1.7304e-04,  1.6103e-01, -4.3892e-02],\n",
      "          [-5.5105e-02, -4.5843e-02,  4.1976e-02,  8.0603e-02, -7.1391e-02],\n",
      "          [ 2.2376e-01,  8.3446e-02,  5.8740e-05, -5.0511e-02,  3.3203e-02],\n",
      "          [ 1.5663e-01,  8.6247e-02, -4.2226e-02, -2.0691e-02, -4.9604e-02]],\n",
      "\n",
      "         [[ 4.3782e-03, -7.1910e-03, -2.6156e-02, -3.2899e-02,  1.2709e-02],\n",
      "          [ 1.6074e-01, -5.3233e-02, -7.5895e-02,  5.7426e-02,  4.7293e-02],\n",
      "          [ 5.3979e-02, -5.0594e-02, -1.2791e-01, -9.8308e-02, -1.2441e-01],\n",
      "          [ 3.1385e-01,  3.2003e-01,  2.2148e-01,  3.9680e-02,  6.6608e-02],\n",
      "          [ 2.8385e-01,  2.7848e-01,  1.4308e-01,  5.8141e-02,  7.2934e-02]],\n",
      "\n",
      "         [[-3.3557e-02,  1.3974e-01, -1.7695e-02,  3.1909e-02,  2.0423e-01],\n",
      "          [ 4.7448e-02, -2.3983e-02,  1.4507e-01, -8.1022e-03,  6.3016e-02],\n",
      "          [ 3.4758e-02,  1.4002e-01,  1.3404e-01, -6.0826e-03, -1.2459e-01],\n",
      "          [-1.4920e-01, -4.3189e-01, -3.1611e-01, -1.5244e-01, -2.5213e-01],\n",
      "          [ 1.4406e-01,  1.0676e-01,  1.6157e-01,  6.7215e-02,  6.1047e-02]],\n",
      "\n",
      "         [[-1.0554e-01, -7.4366e-03,  1.7459e-01,  1.3011e-01,  3.7610e-03],\n",
      "          [-6.5055e-03,  3.2724e-02, -4.8663e-02,  2.8022e-02,  5.4447e-02],\n",
      "          [-6.7098e-02, -2.2199e-01, -3.1549e-01, -6.5678e-02, -7.9665e-02],\n",
      "          [-3.6221e-01, -4.7938e-01, -2.6072e-01, -5.1976e-02, -1.5070e-01],\n",
      "          [-2.4935e-01, -1.6144e-01,  4.0061e-02, -7.0665e-02,  8.7345e-03]],\n",
      "\n",
      "         [[-1.0725e-01, -2.2575e-03, -9.0014e-03,  5.9384e-03,  1.5908e-01],\n",
      "          [-3.5826e-02, -1.0958e-01, -6.6017e-03,  4.2891e-02,  4.2241e-02],\n",
      "          [-8.3147e-02, -2.7910e-01, -3.0904e-01, -1.7216e-01, -1.3851e-01],\n",
      "          [ 7.6326e-02, -3.7003e-02, -7.5506e-02, -8.7459e-02, -9.5344e-03],\n",
      "          [ 2.3327e-01,  1.9935e-01,  1.3545e-01,  1.1368e-01,  1.3261e-01]],\n",
      "\n",
      "         [[ 1.3259e-01,  1.4528e-01,  8.7471e-02,  6.4887e-02,  4.4862e-02],\n",
      "          [-1.0398e-01, -8.2029e-02,  2.9567e-03, -1.8559e-01, -1.9550e-01],\n",
      "          [-3.4273e-01, -2.3336e-01, -2.3787e-02, -2.6331e-01, -1.7721e-01],\n",
      "          [-6.6511e-02,  4.2629e-02,  4.4819e-02,  1.7479e-02,  1.1384e-01],\n",
      "          [ 1.1262e-01, -8.4154e-02,  2.6849e-03, -4.1567e-02, -1.1515e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 6.2711e-02,  1.2384e-01, -8.0628e-02, -2.7216e-02,  1.0054e-01],\n",
      "          [ 9.3481e-02,  3.7648e-02,  5.6533e-02, -5.1688e-03,  6.4125e-02],\n",
      "          [-1.4247e-01, -2.1256e-01,  5.8071e-02, -3.3461e-02, -3.0619e-03],\n",
      "          [ 7.0798e-02, -1.0789e-02, -2.9610e-01, -1.1310e-01,  8.1181e-03],\n",
      "          [ 1.0806e-01,  1.2650e-01,  5.1009e-02, -1.3673e-01, -1.1903e-01]],\n",
      "\n",
      "         [[-7.5493e-02, -1.0034e-01,  8.1292e-02,  1.3127e-01,  1.4428e-01],\n",
      "          [ 6.1305e-02,  1.9436e-01,  1.5478e-01,  2.0344e-01,  6.1822e-02],\n",
      "          [-1.0583e-02, -7.2646e-02, -9.8111e-02, -3.7541e-02, -2.5904e-02],\n",
      "          [ 1.7209e-01,  8.9735e-02, -1.0987e-01,  2.1681e-03, -8.3596e-02],\n",
      "          [ 1.4607e-01,  8.6582e-02,  4.5693e-02, -8.1582e-03, -1.3213e-01]],\n",
      "\n",
      "         [[ 1.1297e-01, -7.5783e-04, -2.4673e-02,  7.6934e-02, -6.3595e-02],\n",
      "          [-3.8841e-02, -8.5611e-02,  5.1573e-02,  1.6824e-01,  2.3193e-02],\n",
      "          [ 1.2333e-01,  1.5152e-01,  2.7957e-01,  1.0331e-01, -2.0795e-03],\n",
      "          [-1.9338e-02,  1.8267e-02, -1.1847e-01, -8.5360e-02,  2.7346e-02],\n",
      "          [ 1.3279e-01, -2.3649e-01, -2.4891e-01,  4.4177e-02,  1.0412e-01]],\n",
      "\n",
      "         [[ 2.0100e-01,  6.5745e-02,  5.1715e-02, -2.2316e-01, -2.3921e-01],\n",
      "          [ 2.2664e-03, -1.5825e-01, -2.6983e-01, -2.5441e-01, -2.1264e-01],\n",
      "          [-4.5940e-02, -4.0516e-02, -6.0457e-03, -1.2890e-01, -1.4482e-01],\n",
      "          [-1.6562e-01, -7.2027e-03, -1.1629e-01, -2.0754e-01,  3.9819e-02],\n",
      "          [-6.0802e-02,  1.0569e-01,  1.2426e-01,  1.6327e-02,  1.7382e-01]],\n",
      "\n",
      "         [[ 1.0831e-01,  1.2661e-02,  9.5440e-02,  2.2079e-01,  1.9202e-01],\n",
      "          [-1.2247e-01,  1.1165e-01,  2.1828e-01,  2.5187e-01,  6.9676e-02],\n",
      "          [-1.4651e-01,  1.2645e-02,  4.9637e-02, -3.8626e-04, -9.4461e-02],\n",
      "          [-2.8019e-02, -1.6747e-01, -1.8262e-01, -1.4467e-01, -3.0807e-04],\n",
      "          [ 8.0638e-03,  2.2118e-02, -5.8065e-02,  4.2697e-02,  3.1464e-03]],\n",
      "\n",
      "         [[-7.5990e-03,  2.3372e-02,  3.7414e-02,  7.9590e-03, -9.7303e-03],\n",
      "          [-3.3260e-02, -2.6186e-02,  4.2867e-03, -1.1063e-01, -8.4392e-02],\n",
      "          [-1.5600e-01, -2.2125e-01, -2.4497e-01, -1.3199e-01, -5.4662e-03],\n",
      "          [-1.6501e-02, -2.1369e-01, -1.2507e-01,  1.1211e-01,  1.1383e-01],\n",
      "          [ 5.0998e-02,  3.8066e-02, -2.4334e-02, -3.6646e-02, -2.9342e-02]]],\n",
      "\n",
      "\n",
      "        [[[-9.1443e-03, -9.3397e-02,  5.9784e-02, -5.7711e-02, -5.0482e-02],\n",
      "          [ 1.0113e-01,  1.2673e-01,  4.2378e-02, -1.9598e-02, -2.7501e-02],\n",
      "          [ 2.0026e-01,  2.3518e-02,  8.3740e-02,  3.4713e-02, -1.5827e-01],\n",
      "          [ 2.1461e-02,  1.6650e-02, -2.5174e-01, -2.9136e-01, -1.5121e-01],\n",
      "          [-1.0476e-01,  7.4061e-02, -3.5041e-02, -1.1239e-01, -1.0886e-01]],\n",
      "\n",
      "         [[ 7.2437e-02, -8.5089e-02, -1.9802e-02, -1.4255e-02, -1.1913e-02],\n",
      "          [ 1.4386e-01, -7.6039e-02,  8.1765e-02, -1.9003e-02,  1.2679e-02],\n",
      "          [ 5.2904e-02,  1.7623e-01,  1.4373e-01,  1.1234e-01,  8.8420e-02],\n",
      "          [-1.1067e-01, -2.3909e-01, -7.3894e-02, -1.6136e-01, -1.0742e-01],\n",
      "          [ 1.0728e-01,  8.0788e-02,  2.0128e-01,  3.2147e-01,  9.4362e-02]],\n",
      "\n",
      "         [[-1.4791e-01, -2.0983e-01, -1.3412e-01, -1.8773e-01, -1.8822e-01],\n",
      "          [-2.0211e-01, -7.9254e-02,  1.3756e-02, -3.5883e-02,  4.5327e-03],\n",
      "          [ 4.8117e-03,  6.3371e-02, -3.2841e-02,  1.7881e-02,  7.1414e-02],\n",
      "          [ 2.3042e-01,  2.1551e-01,  3.2896e-01,  2.3803e-01, -4.2556e-02],\n",
      "          [-1.2790e-01, -9.0491e-02, -1.7660e-01, -2.3202e-01, -1.9430e-01]],\n",
      "\n",
      "         [[ 8.9090e-02,  8.7217e-02,  1.1390e-01,  5.2785e-02,  2.2911e-01],\n",
      "          [-2.0331e-02, -1.7417e-01,  1.5974e-02,  1.0578e-01,  1.9317e-01],\n",
      "          [-1.5637e-01,  1.7642e-01,  1.1868e-01,  2.0566e-01,  1.4183e-01],\n",
      "          [-1.2986e-01, -7.7302e-03,  2.8505e-02,  1.1228e-01, -1.3025e-03],\n",
      "          [-1.7600e-01, -2.4593e-01, -2.5494e-01, -2.6473e-01, -1.1946e-01]],\n",
      "\n",
      "         [[ 7.6249e-02,  1.0501e-02,  9.6499e-02,  3.7788e-02, -2.1179e-03],\n",
      "          [ 6.9398e-03, -9.8156e-03,  1.2449e-01,  1.4373e-01,  1.2215e-01],\n",
      "          [ 9.6661e-02,  1.7303e-01,  1.0428e-01,  2.6878e-01,  1.4907e-01],\n",
      "          [-1.5046e-01, -7.0571e-02,  3.8483e-02, -7.5467e-02, -1.7098e-01],\n",
      "          [-7.7371e-03, -1.3082e-01, -1.6064e-01, -5.6473e-02, -1.6168e-02]],\n",
      "\n",
      "         [[-6.3982e-03, -1.1377e-02, -5.0167e-02,  5.3625e-02, -1.1207e-01],\n",
      "          [-1.0294e-01,  1.3938e-02, -4.8889e-03,  1.2262e-01,  1.1362e-01],\n",
      "          [-1.2097e-01, -2.8593e-02,  3.5700e-02,  4.5873e-02, -1.5885e-01],\n",
      "          [-2.7948e-01, -3.1397e-01, -2.4961e-01, -3.0363e-01, -2.4729e-01],\n",
      "          [-1.5279e-01, -1.9743e-01, -2.1227e-01,  4.8363e-02, -1.2494e-01]]]],\n",
      "       requires_grad=True)\n",
      "conv2.bias torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([-0.0530, -0.0484, -0.0677,  0.1224,  0.0425,  0.0283,  0.1158,  0.1005,\n",
      "         0.0199, -0.0099, -0.1932, -0.0794, -0.1017, -0.0391, -0.0191,  0.1250],\n",
      "       requires_grad=True)\n",
      "fc1.weight torch.Size([120, 256])\n",
      "Parameter containing:\n",
      "tensor([[-0.0664,  0.0001, -0.0846,  ..., -0.0420, -0.0045, -0.0920],\n",
      "        [ 0.0492, -0.0270, -0.0961,  ...,  0.0596,  0.0695, -0.0243],\n",
      "        [ 0.0461, -0.0546,  0.0094,  ..., -0.0465, -0.0315,  0.0210],\n",
      "        ...,\n",
      "        [-0.0194, -0.0551, -0.0506,  ..., -0.0308, -0.0356, -0.0320],\n",
      "        [ 0.0590,  0.0043, -0.0691,  ..., -0.1126, -0.1132, -0.0419],\n",
      "        [-0.0088,  0.0251,  0.0032,  ...,  0.0094,  0.0147,  0.0357]],\n",
      "       requires_grad=True)\n",
      "fc1.bias torch.Size([120])\n",
      "Parameter containing:\n",
      "tensor([ 0.0263,  0.0303,  0.0010, -0.0006, -0.0106,  0.0029, -0.0512,  0.0694,\n",
      "        -0.0077,  0.0010, -0.0927,  0.0037,  0.0521,  0.0377, -0.0861,  0.0614,\n",
      "         0.0188, -0.0463,  0.0515,  0.0051, -0.0421,  0.0347, -0.0659, -0.0178,\n",
      "        -0.0122,  0.1096, -0.0488,  0.0649,  0.0322,  0.0280, -0.0588, -0.0375,\n",
      "         0.1106,  0.0927, -0.0174, -0.0051,  0.0573, -0.0790, -0.0149, -0.0791,\n",
      "        -0.0042, -0.0686, -0.0557, -0.0393,  0.0923, -0.0521, -0.0727, -0.0521,\n",
      "         0.0357, -0.0553,  0.0444, -0.0349, -0.0282, -0.0543, -0.0289, -0.0041,\n",
      "         0.0063,  0.0467, -0.0689,  0.0030,  0.0175, -0.0907, -0.0072, -0.0078,\n",
      "         0.0473, -0.0436, -0.0717, -0.0780, -0.0653,  0.0250, -0.0366,  0.0577,\n",
      "         0.1157,  0.1491, -0.0170, -0.0076,  0.0370, -0.1020, -0.0404,  0.0803,\n",
      "        -0.0526,  0.0116, -0.0616, -0.0044, -0.0194,  0.1234, -0.0203,  0.0732,\n",
      "         0.0319,  0.0450, -0.0303, -0.0446,  0.0277,  0.0786,  0.0277, -0.0459,\n",
      "        -0.0497,  0.0783, -0.0207, -0.0245, -0.0185,  0.0229,  0.0445, -0.0036,\n",
      "         0.0246,  0.1039, -0.0600,  0.0448, -0.0552, -0.0447, -0.0787,  0.0064,\n",
      "        -0.0381, -0.0222,  0.0359, -0.0057,  0.1118,  0.0048, -0.0238, -0.0610],\n",
      "       requires_grad=True)\n",
      "fc2.weight torch.Size([84, 120])\n",
      "Parameter containing:\n",
      "tensor([[-0.1003,  0.0691,  0.0101,  ...,  0.0762, -0.0805, -0.0700],\n",
      "        [ 0.0318,  0.1367, -0.0411,  ..., -0.0286, -0.1380, -0.0149],\n",
      "        [ 0.0178, -0.0642, -0.0027,  ..., -0.0805, -0.2072,  0.0566],\n",
      "        ...,\n",
      "        [ 0.0169, -0.0762, -0.0015,  ..., -0.0583,  0.1052, -0.1240],\n",
      "        [ 0.0225,  0.1613,  0.0247,  ..., -0.0744, -0.0297, -0.0980],\n",
      "        [ 0.0801, -0.0943,  0.0243,  ...,  0.0394, -0.0472, -0.0132]],\n",
      "       requires_grad=True)\n",
      "fc2.bias torch.Size([84])\n",
      "Parameter containing:\n",
      "tensor([ 0.1162,  0.1806, -0.0320,  0.0917,  0.0486, -0.0364, -0.0869, -0.0134,\n",
      "         0.1688,  0.0067,  0.0329,  0.0737,  0.0203, -0.0669, -0.0136, -0.0415,\n",
      "         0.0188, -0.0126,  0.0676,  0.1102,  0.0796,  0.1369, -0.0297,  0.1933,\n",
      "         0.0350, -0.0342, -0.0026,  0.0799,  0.1073, -0.1136,  0.0625,  0.0703,\n",
      "         0.1355, -0.0923,  0.0458, -0.0141, -0.1141, -0.0846,  0.0276, -0.0226,\n",
      "         0.0561,  0.0669, -0.0732,  0.0202, -0.0732, -0.0082,  0.0370,  0.1427,\n",
      "        -0.0051,  0.0455,  0.1343, -0.0004, -0.0915,  0.0138,  0.0220, -0.0867,\n",
      "         0.0644,  0.0293, -0.1152, -0.1168,  0.1014, -0.0576, -0.0947,  0.0347,\n",
      "         0.1447,  0.0467, -0.0187,  0.0432,  0.0897,  0.0090,  0.1096,  0.0765,\n",
      "        -0.0927, -0.1198,  0.0616,  0.0500, -0.1472,  0.0246,  0.0068,  0.0837,\n",
      "         0.0003,  0.0843, -0.1028, -0.0870], requires_grad=True)\n",
      "fc3.weight torch.Size([10, 84])\n",
      "Parameter containing:\n",
      "tensor([[-1.1075e-01, -3.0714e-01, -1.5907e-02, -7.0541e-02, -2.1941e-01,\n",
      "         -3.2538e-01,  1.8899e-01,  1.4345e-01, -7.4459e-02, -1.4170e-01,\n",
      "          4.2213e-02,  7.8088e-02, -1.4080e-01,  1.0251e-02, -1.3275e-01,\n",
      "         -3.5912e-03,  1.5807e-01, -1.4655e-01, -2.8488e-01, -1.5441e-01,\n",
      "         -8.3135e-03,  1.2091e-01,  1.2003e-01,  1.5822e-01,  7.0403e-02,\n",
      "          5.8595e-02,  2.4891e-01, -1.7001e-02, -1.6831e-01, -1.2770e-01,\n",
      "          1.5150e-02,  5.8917e-02, -2.7839e-01, -1.7988e-01,  1.1458e-01,\n",
      "          2.6962e-01, -1.4471e-01, -5.0641e-02, -1.3590e-01,  6.4337e-02,\n",
      "         -2.6517e-01, -5.7161e-02, -1.0530e-01,  7.7029e-02,  5.7630e-03,\n",
      "         -9.3082e-02, -1.1982e-01,  8.7105e-03, -9.0551e-02, -2.1315e-01,\n",
      "          6.8306e-02,  1.1802e-01,  3.3225e-02,  4.1685e-02, -1.2252e-01,\n",
      "         -9.2957e-02,  6.9966e-02,  1.0915e-01, -2.4171e-01, -1.1346e-01,\n",
      "         -1.0571e-01, -7.8311e-02, -1.1944e-01, -6.5614e-02,  6.3368e-02,\n",
      "         -2.8834e-01, -2.4576e-01, -1.7418e-01, -4.0867e-01,  1.8864e-01,\n",
      "          8.0156e-02, -2.5451e-01, -6.8886e-03,  4.3947e-02,  1.2396e-01,\n",
      "          3.4173e-02, -5.3393e-02,  1.5774e-01, -7.6506e-02,  2.2243e-01,\n",
      "          2.2531e-02,  1.4369e-01, -3.3455e-01,  1.1317e-02],\n",
      "        [ 5.3496e-02, -1.2172e-02, -2.0279e-01, -2.1626e-01,  3.2259e-02,\n",
      "         -2.1803e-01,  1.3344e-01,  1.9541e-01, -2.1729e-01,  1.5776e-01,\n",
      "         -1.1311e-01,  1.4164e-01,  1.9996e-02, -1.1137e-01, -2.5445e-01,\n",
      "         -2.3285e-01, -5.4031e-02,  1.4582e-01, -1.3503e-01, -7.5666e-02,\n",
      "         -4.2348e-02, -1.5474e-01,  1.1968e-01, -3.6646e-02, -8.5095e-02,\n",
      "         -1.8806e-01,  2.2295e-02, -1.4886e-01,  1.0220e-01, -4.3669e-02,\n",
      "         -1.6924e-01,  1.1072e-01, -2.5011e-02,  1.4464e-01, -3.2446e-01,\n",
      "         -1.5850e-01,  1.7619e-01,  1.7687e-01, -1.6074e-01, -1.1936e-01,\n",
      "          1.3283e-01, -5.9632e-02, -3.5879e-01,  3.8994e-02, -1.3287e-01,\n",
      "         -1.5629e-01, -2.4940e-01, -1.6647e-01, -1.2252e-01, -2.6558e-02,\n",
      "         -1.2996e-01,  4.8412e-02, -7.9561e-02, -3.2770e-01,  2.4942e-01,\n",
      "          5.4125e-02, -1.1808e-01,  1.6818e-01,  1.5084e-01, -1.7936e-01,\n",
      "         -1.1591e-01, -1.0033e-01,  1.1101e-01,  6.2635e-02,  7.4759e-02,\n",
      "          1.8658e-01,  1.2531e-02,  1.2398e-02, -2.1491e-01, -2.4648e-01,\n",
      "         -2.9463e-02,  1.4612e-01, -6.1342e-03,  2.8452e-02,  1.3363e-01,\n",
      "          2.6788e-02, -2.9209e-02,  1.4217e-01, -2.4726e-01, -2.2963e-01,\n",
      "         -1.8652e-01, -3.5791e-02,  5.2753e-02, -1.0549e-01],\n",
      "        [ 1.7241e-01, -1.5651e-01, -1.5492e-01,  1.1165e-01, -1.6870e-01,\n",
      "          1.8641e-02, -3.6618e-02, -1.3151e-01, -5.4969e-02, -1.4619e-01,\n",
      "         -1.0596e-01, -1.1080e-01, -1.5798e-01,  2.0372e-01,  1.0416e-01,\n",
      "         -1.6334e-01, -1.7644e-01,  1.6807e-02,  1.1251e-01,  4.8121e-02,\n",
      "         -7.2265e-03,  1.4136e-01,  8.0752e-02, -2.5847e-01,  4.4279e-02,\n",
      "         -5.1145e-02, -1.5285e-01,  8.4885e-02, -1.0018e-01, -3.0545e-01,\n",
      "         -2.4290e-01, -1.6881e-02, -2.1017e-01, -1.3090e-01, -1.1372e-01,\n",
      "          1.7939e-01,  6.1056e-02,  4.0741e-02, -1.3070e-01,  8.3014e-02,\n",
      "         -3.1145e-02, -1.6694e-01,  5.1949e-02,  1.3354e-01,  2.2429e-02,\n",
      "         -5.7838e-02,  1.8990e-01,  3.3800e-03, -4.2388e-02,  6.1545e-02,\n",
      "          1.9078e-03,  1.8012e-01,  2.6194e-02, -6.2776e-03, -6.2616e-02,\n",
      "         -3.6745e-02,  1.2644e-01,  6.2306e-02, -4.4540e-02, -1.3474e-01,\n",
      "          1.0688e-02,  5.6815e-02, -3.1750e-02, -2.6940e-01, -1.2413e-01,\n",
      "          1.7625e-01,  1.3550e-01, -8.8205e-02, -2.5815e-01, -2.0627e-02,\n",
      "         -2.7541e-01,  6.9025e-02,  1.1443e-02,  1.3743e-01, -3.5791e-01,\n",
      "         -1.2292e-01, -1.1913e-01, -2.0727e-02,  1.5558e-01,  6.9242e-02,\n",
      "         -1.0357e-01,  8.7897e-02, -7.3124e-02, -3.6731e-02],\n",
      "        [-1.7725e-01, -4.6377e-02,  1.3768e-01, -9.5566e-03,  1.0365e-01,\n",
      "         -2.6854e-02, -1.3515e-01, -2.2008e-01, -6.8752e-02, -4.5886e-02,\n",
      "         -7.7320e-02, -1.1625e-01, -1.1547e-01,  1.4204e-02, -6.8802e-02,\n",
      "         -2.4188e-01,  1.0045e-01,  8.6392e-02,  1.0209e-01,  2.0427e-01,\n",
      "          9.1973e-02, -9.8734e-02,  8.8802e-02, -2.0162e-01, -1.8049e-01,\n",
      "         -7.9648e-02, -2.2130e-01,  1.9887e-01,  1.2792e-01, -2.0973e-01,\n",
      "         -7.8271e-02,  2.0030e-01, -4.6862e-02, -1.6693e-01,  3.1387e-02,\n",
      "         -1.8912e-01, -1.4137e-02, -2.3051e-01, -9.7272e-02, -8.6703e-02,\n",
      "         -1.0899e-01, -1.5379e-01,  1.0441e-01, -6.7743e-02,  2.7479e-02,\n",
      "          2.1996e-01, -1.2606e-01,  1.1112e-01,  1.3858e-01, -3.3401e-02,\n",
      "         -6.1681e-02,  4.4084e-02, -4.0278e-02,  1.2685e-01, -1.0682e-01,\n",
      "         -3.9173e-01, -5.2394e-04, -1.4331e-01, -4.3574e-02,  2.4205e-02,\n",
      "         -1.7258e-01,  4.9279e-02, -1.7011e-01, -2.0061e-01, -4.5994e-02,\n",
      "         -1.1380e-01,  1.2249e-01,  8.7193e-02, -2.4790e-01,  5.3253e-02,\n",
      "          5.2582e-02,  4.4718e-02, -6.9832e-02,  1.3492e-01, -1.6676e-01,\n",
      "          1.7504e-01, -9.6877e-02, -2.8140e-01, -2.1000e-01, -1.5886e-01,\n",
      "          1.0615e-01, -3.0763e-02, -1.7383e-01, -1.2538e-01],\n",
      "        [ 5.6105e-02,  1.4672e-01,  1.4902e-01,  1.5097e-01,  6.8629e-02,\n",
      "          9.1091e-02,  1.6544e-02,  4.0434e-03, -4.2382e-03,  2.3477e-02,\n",
      "         -1.0985e-01, -2.0235e-01, -9.5034e-02, -4.0602e-02,  1.9002e-01,\n",
      "         -1.7493e-01, -2.1602e-01,  1.0789e-02, -2.5623e-01, -1.9183e-01,\n",
      "         -8.9774e-02,  7.2864e-02, -3.3192e-01,  8.5959e-02,  1.9189e-01,\n",
      "          1.0881e-01, -1.9174e-02,  3.2085e-03, -2.0368e-01, -1.4643e-01,\n",
      "          1.4733e-01,  1.7853e-01, -3.0571e-02, -5.7574e-02,  9.2609e-03,\n",
      "         -2.0858e-01, -4.4866e-02, -9.7616e-03, -2.8368e-01, -1.4718e-01,\n",
      "          2.0579e-02,  3.7630e-02,  1.2803e-01,  1.3343e-01, -1.2682e-01,\n",
      "         -1.2624e-01,  1.5162e-01, -3.1754e-01,  1.0970e-01, -7.9935e-02,\n",
      "         -4.8292e-02, -3.4026e-02, -5.4199e-02,  5.5026e-03, -1.4783e-01,\n",
      "         -1.6642e-01, -1.7499e-01, -2.4399e-02,  7.3540e-02,  3.0408e-03,\n",
      "          8.5126e-02,  9.4670e-02,  6.7783e-02,  5.3120e-02, -3.2978e-01,\n",
      "         -1.6420e-02,  1.4490e-02,  7.8611e-02,  2.0368e-01, -1.0882e-01,\n",
      "         -1.8813e-01, -3.1144e-02, -9.2924e-02, -1.4285e-01,  6.4566e-02,\n",
      "         -2.5409e-01, -3.6605e-02, -6.0971e-02, -3.4209e-02,  4.5193e-02,\n",
      "         -2.2704e-01, -2.0198e-01,  6.4990e-02, -1.7784e-01],\n",
      "        [-1.3657e-01,  4.0078e-02, -1.0087e-01, -1.9743e-01,  1.1036e-01,\n",
      "          3.3207e-02, -8.5521e-02, -4.9303e-02, -1.7132e-01,  3.6251e-02,\n",
      "         -8.5829e-02,  1.5361e-02,  9.8261e-02, -6.6384e-03,  1.2603e-01,\n",
      "         -1.1409e-01,  4.1285e-02, -1.0267e-01,  7.9530e-02,  7.8777e-02,\n",
      "         -7.6184e-02, -2.6431e-02, -1.6559e-01, -2.1360e-01, -1.0973e-01,\n",
      "          1.3954e-02,  2.4050e-02, -1.5234e-01, -4.0181e-02, -4.2161e-01,\n",
      "          1.4009e-02, -1.8324e-01,  2.4183e-02, -7.1688e-03,  1.5667e-01,\n",
      "         -8.0782e-02,  2.0583e-01,  3.5677e-02,  1.1449e-01,  6.3968e-02,\n",
      "         -1.5096e-02,  1.3236e-01, -1.6611e-01, -1.9555e-01,  1.6919e-02,\n",
      "          3.2679e-02, -1.1741e-01,  3.3179e-02,  1.4526e-01, -6.2629e-02,\n",
      "         -8.9831e-03, -3.0793e-02, -4.6751e-02,  1.2621e-01, -4.8842e-02,\n",
      "         -6.5277e-02,  7.1059e-02, -1.2841e-01,  1.0059e-01, -7.3369e-02,\n",
      "          1.3170e-01,  3.0023e-02, -3.2904e-01,  9.7207e-02, -2.5108e-02,\n",
      "         -2.3582e-01, -2.3511e-01, -1.0866e-01, -1.2327e-01,  1.7504e-01,\n",
      "          1.1398e-01, -3.3736e-02,  8.7179e-03,  7.9699e-02, -1.1591e-02,\n",
      "          1.1511e-02, -1.0184e-02, -8.2921e-02, -1.3493e-01, -2.6770e-01,\n",
      "          8.7286e-04,  7.3263e-02, -2.9000e-01,  4.7504e-02],\n",
      "        [-4.3616e-03, -2.5845e-02,  5.6484e-02,  1.1167e-01, -3.2308e-01,\n",
      "         -3.3318e-01,  1.5875e-01,  1.8773e-01, -2.5950e-01, -2.3831e-01,\n",
      "          6.7865e-02, -1.2387e-01,  1.5813e-01,  1.4051e-01,  1.1578e-01,\n",
      "          3.9336e-02, -4.2349e-02,  8.6756e-02, -7.6229e-02,  6.8320e-02,\n",
      "         -1.7173e-01, -1.0861e-01, -2.2307e-01,  1.5297e-03, -2.5265e-01,\n",
      "         -1.6077e-01,  2.9119e-02, -3.0816e-01, -2.2244e-01, -3.5252e-01,\n",
      "          1.4503e-01,  1.7351e-01,  5.1521e-02, -9.6249e-02,  1.1482e-01,\n",
      "          3.4469e-02, -6.4018e-02, -3.1734e-03,  1.0094e-01,  1.5521e-01,\n",
      "         -1.3730e-01, -2.0626e-02,  1.4444e-01,  3.9044e-02, -1.2166e-01,\n",
      "          1.0308e-01, -2.9425e-01, -1.3682e-02, -1.8626e-01, -4.2082e-01,\n",
      "         -7.9620e-03, -1.8902e-01, -6.5302e-02,  7.4368e-02, -4.0645e-02,\n",
      "          1.1435e-02, -9.2212e-02,  1.2087e-01, -1.4898e-01,  8.1577e-02,\n",
      "          1.5027e-01,  1.8531e-02, -5.2364e-02,  1.6899e-01, -6.7443e-02,\n",
      "          9.2640e-05, -3.0551e-01, -1.5295e-01, -1.3106e-01,  1.4011e-02,\n",
      "          1.7903e-01,  5.7190e-03, -1.9211e-02, -6.4886e-02, -1.0328e-01,\n",
      "         -2.8584e-01,  4.5274e-02,  4.0614e-02, -2.1581e-01, -3.8442e-03,\n",
      "         -2.7889e-01, -3.5819e-02, -3.8441e-01,  1.1240e-03],\n",
      "        [-1.2642e-01, -3.6938e-02, -1.6404e-01,  1.4240e-01,  1.3066e-01,\n",
      "          1.1929e-01,  1.0056e-01, -8.1407e-02,  1.5337e-01, -3.4051e-02,\n",
      "          6.8479e-02,  1.1461e-01, -2.0798e-01, -2.6384e-01, -1.5080e-01,\n",
      "         -3.7904e-01,  1.3178e-01,  1.4397e-01, -7.9919e-02, -2.0436e-02,\n",
      "          1.6974e-01, -2.8969e-02, -8.1317e-02, -1.3581e-01,  1.6286e-02,\n",
      "          3.4386e-02, -2.8269e-02, -5.2059e-02, -1.0732e-01, -2.6131e-03,\n",
      "          8.0163e-02,  7.6484e-03, -2.7803e-01,  1.2160e-01,  6.4977e-02,\n",
      "         -2.1745e-02, -5.9348e-02, -1.4008e-01, -9.2859e-02, -2.7966e-01,\n",
      "          1.5267e-01,  6.1120e-02,  2.5550e-01, -1.5381e-01, -2.3299e-02,\n",
      "         -1.4691e-01, -4.8003e-02, -1.5405e-01, -2.9258e-01,  2.0038e-01,\n",
      "         -4.5758e-02,  1.8122e-02,  6.8666e-02, -7.7556e-02,  2.3786e-01,\n",
      "         -4.8881e-02, -3.4975e-02, -1.0295e-01,  7.4839e-02, -1.4604e-02,\n",
      "         -2.9295e-01,  5.1219e-02,  8.2521e-03, -1.3831e-01, -1.9816e-01,\n",
      "          2.0213e-01,  5.0953e-02,  7.4260e-02,  8.2429e-02,  1.7627e-01,\n",
      "         -3.5988e-01, -2.0958e-02,  2.2032e-02,  9.2943e-02, -9.6921e-03,\n",
      "          5.7165e-02, -5.5844e-02, -3.0084e-02,  1.2824e-01, -2.4146e-01,\n",
      "          1.4200e-02,  5.6289e-02,  1.1056e-01,  3.4664e-03],\n",
      "        [ 2.0545e-01,  9.2816e-02, -1.9653e-02, -8.0618e-02, -8.0895e-02,\n",
      "         -1.2873e-01, -2.1359e-02, -1.1236e-01,  1.4203e-01, -9.4647e-02,\n",
      "          3.8013e-03,  1.1996e-01,  7.2150e-02, -2.5303e-01, -1.0984e-01,\n",
      "         -1.6446e-01, -2.5828e-02,  4.8914e-02,  1.3614e-01,  7.5985e-02,\n",
      "          4.6264e-02,  1.5064e-01,  3.0002e-03,  1.5631e-01, -6.7404e-03,\n",
      "         -1.3846e-01,  2.0354e-02,  8.9572e-02,  1.1709e-01, -4.3811e-01,\n",
      "         -1.4467e-01, -1.1895e-01,  7.1941e-02, -2.5575e-01, -9.1754e-02,\n",
      "         -9.6906e-02, -1.1185e-01, -5.6467e-02,  1.6611e-01, -3.3620e-03,\n",
      "         -1.7105e-02,  1.3304e-01, -1.1819e-01,  1.0018e-01, -3.2649e-03,\n",
      "         -2.0324e-01,  4.0167e-02,  1.0381e-01, -1.5138e-02, -3.8262e-03,\n",
      "          6.3798e-02, -2.8225e-01, -3.5307e-02, -5.5500e-02, -1.9749e-01,\n",
      "          7.9196e-02, -1.1473e-02, -2.1389e-01, -3.6045e-01, -6.7707e-02,\n",
      "          7.7132e-02,  1.0631e-02, -2.9859e-01, -1.0263e-02,  2.0096e-01,\n",
      "         -7.1483e-02,  6.0438e-02,  1.2839e-01,  9.6881e-03, -2.3718e-01,\n",
      "          3.9961e-02,  8.7176e-03, -1.1473e-01, -2.2269e-01,  4.8786e-02,\n",
      "         -1.7436e-01, -1.6875e-01, -1.9566e-02, -1.3076e-01, -1.1621e-01,\n",
      "         -1.5796e-01,  8.4269e-02, -2.1410e-01,  4.1018e-02],\n",
      "        [-1.3214e-01,  3.3070e-03, -7.5688e-02,  2.9566e-02,  7.5258e-02,\n",
      "          1.3912e-01, -1.4234e-01, -8.9839e-02,  6.5372e-02,  1.6084e-02,\n",
      "         -3.1775e-02,  1.4792e-01, -3.9295e-02,  5.3755e-02, -4.5567e-03,\n",
      "         -1.3804e-01,  2.6864e-02, -3.4131e-01, -3.0207e-01, -2.4960e-01,\n",
      "         -3.9020e-02, -2.2124e-01,  1.7323e-02,  8.0868e-02,  9.1782e-02,\n",
      "          1.9853e-01, -2.4106e-01,  1.0337e-01, -1.2897e-01, -5.5849e-02,\n",
      "          1.3030e-01, -2.9810e-01,  1.1386e-01, -2.2275e-01,  1.6253e-02,\n",
      "          1.4437e-01, -2.3497e-01, -3.5954e-01, -1.4960e-01,  1.0968e-02,\n",
      "         -4.9615e-02, -1.0954e-01, -3.1305e-01,  3.6159e-02, -1.5230e-02,\n",
      "          8.3226e-02,  1.3083e-01,  2.3661e-02,  2.6577e-03, -7.3925e-03,\n",
      "         -1.9061e-01, -1.3151e-01,  8.4528e-02, -2.7917e-02, -1.4558e-01,\n",
      "         -4.8502e-01,  1.2766e-01,  7.0356e-02,  1.2850e-01, -1.7862e-01,\n",
      "          1.2499e-01, -1.1802e-02,  2.0785e-01,  1.1881e-01, -1.4722e-01,\n",
      "         -1.0646e-01,  2.2203e-02,  8.4635e-02,  1.5208e-01, -1.0425e-01,\n",
      "          4.7275e-02,  1.3984e-01, -2.4243e-02, -1.1520e-02, -8.1748e-03,\n",
      "          1.1469e-01,  1.5408e-02, -2.6004e-01, -2.1961e-02,  7.3806e-02,\n",
      "          1.2950e-01, -1.8960e-01, -2.6018e-02, -2.1769e-03]],\n",
      "       requires_grad=True)\n",
      "fc3.bias torch.Size([10])\n",
      "Parameter containing:\n",
      "tensor([-0.1480, -0.1316,  0.0920, -0.0718,  0.0426,  0.0561, -0.1572,  0.0331,\n",
      "         0.0918, -0.0123], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "    print(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
